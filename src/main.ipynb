{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IAA - PRÀCTICA: MAIN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Instal·lar llibreries necessàries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../assets/requirements.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importar llibreries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_dependencies():\n",
    "\tglobal pd, np, plt, sns, skl\n",
    "\n",
    "\timport pandas as pd\n",
    "\timport numpy as np\n",
    "\timport matplotlib.pyplot as plt\n",
    "\timport seaborn as sns\n",
    "\timport sklearn as skl\n",
    "\n",
    "import_dependencies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Llegir les dades (Cirrhosis Dataset)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(save_to_csv: bool = True):\n",
    "\tglobal data\n",
    "\tfrom ucimlrepo import fetch_ucirepo \n",
    "\t\n",
    "\t# Fetch dataset\n",
    "\tcirrhosis_patient_survival_prediction = fetch_ucirepo(id=878)\n",
    "\n",
    "\tdata = pd.DataFrame(cirrhosis_patient_survival_prediction.data.original)\n",
    "\n",
    "\tif save_to_csv:\n",
    "\t\t# Guardem el dataset per poder-lo visualitzar sencer\n",
    "\t\tdata.to_csv('../assets/data/raw_cirrhosis.csv', index=False)\n",
    "\n",
    "load_dataset(save_to_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Informació del dataset inicial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preprocessing inicial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_preprocessing(data: pd.DataFrame, save_to_csv: bool = True):\n",
    "\t\"\"\"\n",
    "\tReemplaça els valors 'NaNN' per NaN, assigna els tipus de dades correctes a cada columna i renombra les classes d'algunes variables per una millor comprensió.\n",
    "\t\"\"\"\n",
    "\t# Reemplaçar l'string 'NaNN' per NaN\n",
    "\tdata.replace(to_replace=['NaNN', '', pd.NA], value=np.nan, inplace=True)\n",
    "\n",
    "\t# Assignem els tipus de dades correctes a cada columna\n",
    "\tint64_variables = ['N_Days', 'Age', 'Cholesterol', 'Copper', 'Tryglicerides', 'Platelets']\n",
    "\tfloat64_variables = ['Bilirubin', 'Albumin', 'Alk_Phos', 'SGOT', 'Prothrombin']\n",
    "\tcategory_variables = ['ID', 'Status', 'Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema', 'Stage']\n",
    "\tboolean_variables = ['Ascites', 'Hepatomegaly', 'Spiders']\n",
    "\n",
    "\tdata[int64_variables] = data[int64_variables].astype('Int64')\n",
    "\tdata[float64_variables] = data[float64_variables].astype('float64')\n",
    "\tdata[category_variables] = data[category_variables].astype('category')\n",
    "\n",
    "\n",
    "\t# Renombrem les classes d'algunes variables per una millor comprensió\n",
    "\tdata['Status'] = data['Status'].replace({'D': 'Dead', 'C': 'Alive', 'CL': 'LiverTransplant'})\n",
    "\tdata[boolean_variables] = data[boolean_variables].replace({'Y': 1, 'N': 0})\n",
    "\tdata['Edema'] = data['Edema'].replace({'N': 'NoEdema', 'S': 'EdemaResolved', 'Y': 'EdemaPersistent'})\n",
    "\n",
    "\tif save_to_csv:\n",
    "\t\t# Guardem el dataset\n",
    "\t\tdata.to_csv('../assets/data/initial_preprocessing_cirrhosis.csv', index=False)\n",
    "\n",
    "initial_preprocessing(data=data, save_to_csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Anàlisis inicial de les variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estudi de les variables numèriques\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estadístiques de les variables categòriques\n",
    "data.describe(include='category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_vars_histograms(data: pd.DataFrame):\n",
    "    # Visualització de les distribucions de les variables numèriques en una sola figura\n",
    "    numerical_columns = data.select_dtypes(include=['Int64', 'float64']).columns\n",
    "\n",
    "    num_rows = int(np.ceil(len(numerical_columns) / 2))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, num_rows * 4))\n",
    "\n",
    "    for i, col in enumerate(numerical_columns):\n",
    "        ax = fig.add_subplot(num_rows, 2, i + 1)\n",
    "        \n",
    "        sns.histplot(data[col], edgecolor=\"k\", linewidth=1.5, kde=True)\n",
    "        \n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        \n",
    "        ax.set_title(f'Distribució de la variable numèrica {col}')\n",
    "        ax.set_xlabel(col)\n",
    "        ax.set_ylabel('Freqüència')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "numerical_vars_histograms(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_vars_countplots(data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Visualització de les distribucions de les variables categòriques en una sola figura (menys ID).\n",
    "    \"\"\"\n",
    "    # Visualització de les distribucions de les variables categòriques en una sola figura (menys ID)\n",
    "    categorical_columns = data.select_dtypes(include=['category']).columns.drop(['ID'])\n",
    "    num_rows = int(np.ceil(len(categorical_columns) / 2))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, num_rows * 4))\n",
    "\n",
    "    for i, col in enumerate(categorical_columns):\n",
    "        ax = fig.add_subplot(num_rows, 2, i + 1)\n",
    "        \n",
    "        sns.countplot(data=data, x=col, ax=ax, hue=col, legend=False)\n",
    "        \n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        \n",
    "        ax.set_title(f'Distribució de la variable categòrica {col}')\n",
    "        ax.set_xlabel(col)\n",
    "        ax.set_ylabel('Quantitat')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "categorical_vars_countplots(data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tractament d'outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_iqr_factors(data: pd.DataFrame, factors: list = [1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5]):\n",
    "\t\"\"\"\n",
    "\tCompara diferents factors que multipliquen al IQR per a determinar els outliers i realitza un gràfic evolutiu per comparar-los.\n",
    "\t\"\"\"\n",
    "\tnumerical_columns = data.select_dtypes(include=['Int64', 'float64']).columns\n",
    "\n",
    "\tplt.figure(figsize=(10, 6))\n",
    "\n",
    "\t# Dictionary to store outlier percentages for each factor and column\n",
    "\toutlier_percentages = {col: [] for col in numerical_columns}\n",
    "\ttotal_percentages = [set() for _ in range(len(factors))]\n",
    "\n",
    "\tfor col in numerical_columns:\n",
    "\t\tQ1 = data[col].quantile(0.25)\n",
    "\t\tQ3 = data[col].quantile(0.75)\n",
    "\t\tIQR = Q3 - Q1\n",
    "\n",
    "\t\tfor f_id, factor in enumerate(factors):\n",
    "\t\t\toutliers_mask = ((data[col] < (Q1 - factor * IQR)) | (data[col] > (Q3 + factor * IQR)))\n",
    "\t\t\ttotal_percentages[f_id].update(data.index[outliers_mask])\n",
    "\t\t\toutliers_percentage = np.mean(outliers_mask) * 100\n",
    "\t\t\toutlier_percentages[col].append(outliers_percentage)\n",
    "\n",
    "\ttotal_percentages = [(len(outliers) / len(data)) * 100 for outliers in total_percentages]\n",
    "\t\t\t\n",
    "\t# Plotting the results\n",
    "\tfor col, percentages in outlier_percentages.items():\n",
    "\t\tplt.plot(factors, percentages, label=col)\n",
    "\tplt.plot(factors, total_percentages, label='Total', linestyle='--', color='black')\n",
    "\n",
    "\tplt.xlabel('Factor multiplicatiu del IQR')\n",
    "\tplt.ylabel('Percentage d\\'outliers (%)')\n",
    "\tplt.title('Percentatge d\\'outliers de cada variable numèrica per a diferents factors multiplicatius del IQR')\n",
    "\tplt.legend()\n",
    "\tplt.grid(True)\n",
    "\tplt.show()\n",
    "\n",
    "compare_iqr_factors(data=data, factors=[1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datset amb outliers: 418 files i 20 columnes.\n",
      "Nombre total d'outliers únics eliminats: 70 (16.75% de tot el dataset).\n",
      "Dataset sense outliers: 348 files i 20 columnes.\n"
     ]
    }
   ],
   "source": [
    "def delete_outliers(data: pd.DataFrame, factor: float = 1.5, plots: bool = True, save_to_csv: bool = True):\n",
    "    \"\"\"\n",
    "    Funció que detecta, visualitza i elimina els outliers d'un dataset. El factor multiplica el IQR per a determinar quins valors són outliers.\n",
    "    \"\"\"\n",
    "    # Detecció, visualització i eliminació d'outliers\n",
    "    numerical_columns = data.select_dtypes(include=['Int64', 'float64']).columns\n",
    "\n",
    "    outliers_indices = []\n",
    "\n",
    "    for col in numerical_columns:\n",
    "        Q1 = data[col].quantile(0.25)\n",
    "        Q3 = data[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers_mask = ((data[col] < (Q1 - factor * IQR)) | (data[col] > (Q3 + factor * IQR)))\n",
    "        outliers = data[col][outliers_mask]\n",
    "        non_outliers = data[col][~outliers_mask]\n",
    "\n",
    "        outliers_indices.extend(data[col][outliers_mask].index.tolist())\n",
    "        \n",
    "        if plots:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(8, 5))\n",
    "\n",
    "            # Boxplot amb els outliers originals\n",
    "            sns.boxplot(ax=axes[0], y=data[col], orient='v')\n",
    "            axes[0].scatter(x=[0]*len(outliers), y=outliers, color='red', marker='o')\n",
    "            axes[0].set_title(f'Boxplot de {col} amb outliers ({factor}x IQR)')\n",
    "            \n",
    "            percent_outliers = len(outliers) / data.shape[0] * 100\n",
    "            axes[0].text(0.5, -0.1, f'Outliers ({factor}x IQR): {len(outliers)} ({percent_outliers:.2f}%)', \n",
    "                        ha='center', va='center', transform=axes[0].transAxes)\n",
    "            \n",
    "            # Boxplot sense els outliers\n",
    "            sns.boxplot(ax=axes[1], y=non_outliers, orient='v')\n",
    "            axes[1].set_title(f'Boxplot de {col} sense outliers')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    unique_outliers = len(set(outliers_indices))\n",
    "\n",
    "    print(f\"Datset amb outliers: {data.shape[0]} files i {data.shape[1]} columnes.\")\n",
    "    print(f\"Nombre total d'outliers únics eliminats: {unique_outliers} ({unique_outliers / data.shape[0] * 100:.2f}% de tot el dataset).\")\n",
    "\n",
    "    # Eliminació d'outliers\n",
    "    data.drop(list(set(outliers_indices)), inplace=True)\n",
    "    \n",
    "    print(f\"Dataset sense outliers: {data.shape[0]} files i {data.shape[1]} columnes.\")\n",
    "\n",
    "    if save_to_csv:\n",
    "        # Guardem el dataset\n",
    "        data.to_csv('../assets/data/no_outliers_cirrhosis.csv', index=False)\n",
    "\n",
    "delete_outliers(data=data, factor=3, plots=False, save_to_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Recodificació de variables categòriques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_variables(data: pd.DataFrame, save_to_csv: bool = True):\n",
    "    \"\"\"\n",
    "    Codifica les variables categòriques que calgui per a poder-les utilitzar en els models de ML. \n",
    "    A més, guarda el mapping per a poder decodificar-les.\n",
    "    Els NaNs es mantenen (en comptes de considerar-los una classe més) per poder imputar-los posteriorment.\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.impute import SimpleImputer\n",
    "\n",
    "    global ohe_mapping\n",
    "\n",
    "    columns_to_encode = ['Drug', 'Sex', 'Edema', 'Stage'] # Sense la variable 'Status' perquè és la target i, a més, no té valors NaN\n",
    "\n",
    "    na_indexs_per_old_encoded_column = {col: set(data[data[col].isna()].index) for col in columns_to_encode} # Guardem els indexs dels NaNs per a cada columna a codificar\n",
    "    new_encoded_columns_per_old_encoded_column = {col: set() for col in columns_to_encode} # Guardem les classes de cada columna a codificar\n",
    "\n",
    "    # Imputem els NaNs per evitar que es crein columnes innecessàries al fer el OneHotEncoding. Després tornarem a inserir els NaNs\n",
    "    data[columns_to_encode] = SimpleImputer(strategy='most_frequent').fit_transform(data[columns_to_encode])\n",
    "\n",
    "    ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "    data_encoded = ohe.fit_transform(data[columns_to_encode])\n",
    "    encoded_columns = ohe.get_feature_names_out(columns_to_encode)\n",
    "\n",
    "    # Guardem el mapping per a poder decodificar les variables\n",
    "    ohe_mapping = {}\n",
    "    for i, col in enumerate(columns_to_encode):\n",
    "        for category in ohe.categories_[i]:\n",
    "            new_encoded_column_name = f\"{col}_{category}\"\n",
    "            ohe_mapping[new_encoded_column_name] = (col, category)\n",
    "            new_encoded_columns_per_old_encoded_column[col].add(new_encoded_column_name)\n",
    "\n",
    "    data[encoded_columns] = data_encoded\n",
    "    data.drop(columns_to_encode, axis=1, inplace=True)\n",
    "\n",
    "    # Tornem a posar els NaNs per poder imputar-los\n",
    "    for col in columns_to_encode:\n",
    "        for na_index in na_indexs_per_old_encoded_column[col]:\n",
    "            for new_column in new_encoded_columns_per_old_encoded_column[col]:\n",
    "                data.loc[na_index, new_column] = np.nan\n",
    "\n",
    "    if save_to_csv:\n",
    "        # Guardem el dataset\n",
    "        data.to_csv('../assets/data/encoded_cirrhosis.csv', index=False)\n",
    "\n",
    "encode_variables(data=data, save_to_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Partició del dataset en train/test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data: pd.DataFrame, test_size: float = 0.15, random_state: int = 42):\n",
    "\t\"\"\"\n",
    "\tParticiona el dataset en train i test.\n",
    "\t\"\"\"\n",
    "\tglobal X_train, y_train, test\n",
    "\n",
    "\tfrom sklearn.model_selection import train_test_split\n",
    "\n",
    "\ttrain, test = train_test_split(data, test_size=test_size, random_state=random_state)\n",
    "\n",
    "\tprint(f\"Train shape: {train.shape}\")\n",
    "\tprint(f\"Test shape: {test.shape}\")\n",
    "\n",
    "\t# 'Status' és la variable target\n",
    "\tX_train = train.drop(columns=['Status'])\n",
    "\ty_train = train['Status']\n",
    "\n",
    "split_dataset(data=data, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imputar els valors faltants (Missings)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Drug': ['Drug_D-penicillamine', 'Drug_Placebo'], 'Sex': ['Sex_F', 'Sex_M'], 'Edema': ['Edema_EdemaPersistent', 'Edema_EdemaResolved', 'Edema_NoEdema'], 'Stage': ['Stage_1.0', 'Stage_2.0', 'Stage_3.0', 'Stage_4.0']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cai Selvas Sala\\AppData\\Local\\Temp\\ipykernel_14520\\1957590651.py:12: FutureWarning: The behavior of DataFrame.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  data[orig_col] = data[new_cols].idxmax(axis=1).apply(lambda new_col_name: new_col_name.split('_')[-1] if new_col_name != np.nan else np.nan)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Cai Selvas Sala\\GIA_UPC\\2nC\\1rQ\\IAA\\Practica\\src\\main.ipynb Cell 32\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y131sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         \u001b[39m# Eliminar les columnes codificades\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y131sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         data\u001b[39m.\u001b[39mdrop(new_cols, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y131sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m decode_variables(data\u001b[39m=\u001b[39;49mdata, ohe_mapping\u001b[39m=\u001b[39;49mohe_mapping)\n",
      "\u001b[1;32mc:\\Users\\Cai Selvas Sala\\GIA_UPC\\2nC\\1rQ\\IAA\\Practica\\src\\main.ipynb Cell 32\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y131sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Creem les columnes decodificades\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y131sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m orig_col, new_cols \u001b[39min\u001b[39;00m inv_ohe_mapping\u001b[39m.\u001b[39mitems():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y131sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# Extraure la columna amb el valor 1 (hot)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y131sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     data[orig_col] \u001b[39m=\u001b[39m data[new_cols]\u001b[39m.\u001b[39;49midxmax(axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m new_col_name: new_col_name\u001b[39m.\u001b[39;49msplit(\u001b[39m'\u001b[39;49m\u001b[39m_\u001b[39;49m\u001b[39m'\u001b[39;49m)[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m] \u001b[39mif\u001b[39;49;00m new_col_name \u001b[39m!=\u001b[39;49m np\u001b[39m.\u001b[39;49mnan \u001b[39melse\u001b[39;49;00m np\u001b[39m.\u001b[39;49mnan)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y131sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# Eliminar les columnes codificades\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y131sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     data\u001b[39m.\u001b[39mdrop(new_cols, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Cai Selvas Sala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:4765\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4625\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4626\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4627\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4632\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4633\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4634\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4635\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4636\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4756\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4757\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4758\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4759\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   4760\u001b[0m         func,\n\u001b[0;32m   4761\u001b[0m         convert_dtype\u001b[39m=\u001b[39;49mconvert_dtype,\n\u001b[0;32m   4762\u001b[0m         by_row\u001b[39m=\u001b[39;49mby_row,\n\u001b[0;32m   4763\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   4764\u001b[0m         kwargs\u001b[39m=\u001b[39;49mkwargs,\n\u001b[1;32m-> 4765\u001b[0m     )\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Users\\Cai Selvas Sala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1201\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1198\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_compat()\n\u001b[0;32m   1200\u001b[0m \u001b[39m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1201\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\Cai Selvas Sala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1281\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1275\u001b[0m \u001b[39m# row-wise access\u001b[39;00m\n\u001b[0;32m   1276\u001b[0m \u001b[39m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1277\u001b[0m \u001b[39m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1278\u001b[0m \u001b[39m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1279\u001b[0m \u001b[39m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1280\u001b[0m action \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj\u001b[39m.\u001b[39mdtype, CategoricalDtype) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1281\u001b[0m mapped \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_map_values(\n\u001b[0;32m   1282\u001b[0m     mapper\u001b[39m=\u001b[39;49mcurried, na_action\u001b[39m=\u001b[39;49maction, convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype\n\u001b[0;32m   1283\u001b[0m )\n\u001b[0;32m   1285\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1286\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1287\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mReturning a DataFrame from Series.apply when the supplied function \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1288\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mreturns a Series is deprecated and will be removed in a future \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1291\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m   1292\u001b[0m     )  \u001b[39m# GH52116\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Cai Selvas Sala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39mmap(mapper, na_action\u001b[39m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[39mreturn\u001b[39;00m algorithms\u001b[39m.\u001b[39;49mmap_array(arr, mapper, na_action\u001b[39m=\u001b[39;49mna_action, convert\u001b[39m=\u001b[39;49mconvert)\n",
      "File \u001b[1;32mc:\\Users\\Cai Selvas Sala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\algorithms.py:1812\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1810\u001b[0m values \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   1811\u001b[0m \u001b[39mif\u001b[39;00m na_action \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1812\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39;49mmap_infer(values, mapper, convert\u001b[39m=\u001b[39;49mconvert)\n\u001b[0;32m   1813\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1814\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1815\u001b[0m         values, mapper, mask\u001b[39m=\u001b[39misna(values)\u001b[39m.\u001b[39mview(np\u001b[39m.\u001b[39muint8), convert\u001b[39m=\u001b[39mconvert\n\u001b[0;32m   1816\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2917\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Cai Selvas Sala\\GIA_UPC\\2nC\\1rQ\\IAA\\Practica\\src\\main.ipynb Cell 32\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y131sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Creem les columnes decodificades\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y131sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m orig_col, new_cols \u001b[39min\u001b[39;00m inv_ohe_mapping\u001b[39m.\u001b[39mitems():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y131sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# Extraure la columna amb el valor 1 (hot)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y131sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     data[orig_col] \u001b[39m=\u001b[39m data[new_cols]\u001b[39m.\u001b[39midxmax(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m new_col_name: new_col_name\u001b[39m.\u001b[39;49msplit(\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m new_col_name \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mnan \u001b[39melse\u001b[39;00m np\u001b[39m.\u001b[39mnan)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y131sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# Eliminar les columnes codificades\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y131sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     data\u001b[39m.\u001b[39mdrop(new_cols, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "def decode_variables(data: pd.DataFrame, ohe_mapping):\n",
    "    # Invertir el mapping per facilitar la decodificació\n",
    "    inv_ohe_mapping = {}\n",
    "    for new_col, (orig_col, _) in ohe_mapping.items():\n",
    "        inv_ohe_mapping.setdefault(orig_col, []).append(new_col)\n",
    "\n",
    "    print(inv_ohe_mapping)\n",
    "\n",
    "    # Creem les columnes decodificades\n",
    "    for orig_col, new_cols in inv_ohe_mapping.items():\n",
    "        # Extraure la columna amb el valor 1 (hot)\n",
    "        data[orig_col] = data[new_cols].idxmax(axis=1)\n",
    "        .apply(lambda new_col_name: new_col_name.split('_')[-1] if new_col_name != np.nan else np.nan)\n",
    "        # CONTINUAR AQUI. CAL PASSAR ELS VALORS DE COLUMNES ANTIGUES A NOUS\n",
    "        # Eliminar les columnes codificades\n",
    "        data.drop(new_cols, axis=1, inplace=True)\n",
    "\n",
    "decode_variables(data=data, ohe_mapping=ohe_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.impute import KNNImputer, IterativeImputer, SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def mejor_imputacion(X_train, y_train):\n",
    "    na_columns = X_train.isna().any()\n",
    "\n",
    "    numerical_columns = X_train.select_dtypes(include=['Int64', 'float64']).columns\n",
    "    categorical_columns = X_train.select_dtypes(include=['category']).columns.drop(['ID'])\n",
    "\n",
    "    MixedImputer = ColumnTransformer([\n",
    "        ('imputer', SimpleImputer(strategy='mean'), numerical_columns),\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent'), categorical_columns)\n",
    "    ])\n",
    "\n",
    "    imputaciones = {\n",
    "        'mixed': MixedImputer, # Mean per numèriques, most_frequent per categòriques\n",
    "        'knn-3': KNNImputer(n_neighbors=3),\n",
    "        'knn-5': KNNImputer(n_neighbors=5),\n",
    "        'iterative-10': IterativeImputer(max_iter=10, random_state=42),\n",
    "        'iterative-20': IterativeImputer(max_iter=20, random_state=42),\n",
    "    }\n",
    "\n",
    "    scores = {}\n",
    "    best_score = float('inf')\n",
    "    best_imputer = None\n",
    "\n",
    "    # Generar valores NA artificialmente\n",
    "    X_missing = X_train.copy()\n",
    "    for col in X_missing.columns:\n",
    "        X_missing.loc[X_missing.sample(frac=0.1).index, col] = np.nan\n",
    "\n",
    "    for name, imputer in imputaciones.items():\n",
    "        X_imputed = imputer.fit_transform(X_missing)\n",
    "        score = mean_squared_error(X_train, X_imputed)\n",
    "\n",
    "        scores[name] = score\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_imputer = name\n",
    "\n",
    "    return best_imputer, scores\n",
    "\n",
    "mejor_imputador, puntuaciones = mejor_imputacion(X_train, y_train)\n",
    "\n",
    "\n",
    "def cross_validation_con_imputacion(X, y):\n",
    "    kf = KFold(n_splits=5)  # Ajustar según necesidades\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        imputacion = mejor_imputacion(X_train, y_train)\n",
    "        # Aplicar la mejor imputación y entrenar el modelo...\n",
    "        # Evaluar el modelo en X_test, y_test...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missings(data: pd.DataFrame, save_to_csv: bool = True):\n",
    "    from sklearn.impute import KNNImputer\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "\n",
    "    categorical_columns = data.select_dtypes(include=['category']).columns.drop(['ID'])\n",
    "\n",
    "    # Creación del transformador para la codificación One-Hot\n",
    "    one_hot_encoder = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    # Aplicación de la codificación One-Hot\n",
    "    data_encoded = one_hot_encoder.fit_transform(data)\n",
    "\n",
    "    # Creación del imputador KNN\n",
    "    knn_imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "    # Imputación de los datos codificados\n",
    "    data_imputed = knn_imputer.fit_transform(data_encoded)\n",
    "\n",
    "    # Convirtiendo de nuevo a un DataFrame de pandas (opcional, dependiendo de cómo necesites usar los datos)\n",
    "    # Nota: La conversión de nuevo a DataFrame requiere asignar nombres a las columnas\n",
    "    # Esto puede ser complejo debido a la codificación One-Hot\n",
    "    data_imputed_df = pd.DataFrame(data_imputed)\n",
    "\n",
    "    # Mostrando las primeras filas de los datos imputados\n",
    "    data_imputed_df.head()\n",
    "\n",
    "    if save_to_csv:\n",
    "        # Guardem el dataset\n",
    "        data_imputed_df.to_csv('../assets/data/imputed_cirrhosis.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "num_columns = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "cat_columns = X_train.select_dtypes(include=['object']).columns\n",
    "imputer_num = SimpleImputer(strategy='median')\n",
    "imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "X_train[num_columns] = imputer_num.fit_transform(X_train[num_columns])\n",
    "X_train[cat_columns] = imputer_cat.fit_transform(X_train[cat_columns])\n",
    "X_test[num_columns] = imputer_num.transform(X_test[num_columns])\n",
    "X_test[cat_columns] = imputer_cat.transform(X_test[cat_columns])\n",
    "\n",
    "# Codificación one-hot de variables categóricas\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "X_train_encoded = pd.DataFrame(encoder.fit_transform(X_train[cat_columns]))\n",
    "X_train_encoded.columns = encoder.get_feature_names_out(cat_columns)\n",
    "X_train = X_train.drop(cat_columns, axis=1)\n",
    "X_train = pd.concat([X_train, X_train_encoded], axis=1)\n",
    "X_test_encoded = pd.DataFrame(encoder.transform(X_test[cat_columns]))\n",
    "X_test_encoded.columns = encoder.get_feature_names_out(cat_columns)\n",
    "X_test = X_test.drop(cat_columns, axis=1)\n",
    "X_test = pd.concat([X_test, X_test_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1r Model: K-Nearest Neighbors (KNN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2n Model: Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3r Model: Support Vector Machine (SVM)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
