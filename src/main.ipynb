{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IAA - PRÀCTICA: MAIN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Instal·lar llibreries necessàries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../assets/requirements.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importar llibreries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_dependencies():\n",
    "\tglobal pd, np, plt, sns, skl\n",
    "\n",
    "\timport pandas as pd\n",
    "\timport numpy as np\n",
    "\timport matplotlib.pyplot as plt\n",
    "\timport seaborn as sns\n",
    "\timport sklearn as skl\n",
    "\n",
    "import_dependencies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Llegir les dades (Cirrhosis Dataset)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(save_to_csv: bool = True):\n",
    "\tglobal data\n",
    "\tfrom ucimlrepo import fetch_ucirepo \n",
    "\t\n",
    "\t# Fetch dataset\n",
    "\tcirrhosis_patient_survival_prediction = fetch_ucirepo(id=878)\n",
    "\n",
    "\tdata = pd.DataFrame(cirrhosis_patient_survival_prediction.data.original)\n",
    "\n",
    "\tif save_to_csv:\n",
    "\t\t# Guardem el dataset per poder-lo visualitzar sencer\n",
    "\t\tdata.to_csv('../assets/data/raw_cirrhosis.csv', index=False)\n",
    "\n",
    "#load_dataset(save_to_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Informació del dataset inicial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preprocessing inicial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_preprocessing(data: pd.DataFrame, save_to_csv: bool = True):\n",
    "\t\"\"\"\n",
    "\tReemplaça els valors 'NaNN' per NaN, assigna els tipus de dades correctes a cada columna i renombra les classes d'algunes variables per una millor comprensió.\n",
    "\t\"\"\"\n",
    "\t# Reemplaçar l'string 'NaNN' per NaN\n",
    "\tdata.replace(to_replace=['NaNN', '', pd.NA], value=np.nan, inplace=True)\n",
    "\n",
    "\t# Assignem els tipus de dades correctes a cada columna\n",
    "\tint64_variables = ['N_Days', 'Age', 'Cholesterol', 'Copper', 'Tryglicerides', 'Platelets']\n",
    "\tfloat64_variables = ['Bilirubin', 'Albumin', 'Alk_Phos', 'SGOT', 'Prothrombin']\n",
    "\tcategory_variables = ['ID', 'Status', 'Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema', 'Stage']\n",
    "\tboolean_variables = ['Ascites', 'Hepatomegaly', 'Spiders']\n",
    "\n",
    "\tdata[int64_variables] = data[int64_variables].astype('Int64')\n",
    "\tdata[float64_variables] = data[float64_variables].astype('float64')\n",
    "\tdata[category_variables] = data[category_variables].astype('category')\n",
    "\n",
    "\tglobal original_column_types\n",
    "\n",
    "\toriginal_column_types = {col: str(data[col].dtype) for col in data.columns} # Guardem els tipus de dades de cada columna\n",
    "\n",
    "\t# Renombrem les classes d'algunes variables per una millor comprensió\n",
    "\tdata['Status'] = data['Status'].replace({'D': 'Dead', 'C': 'Alive', 'CL': 'LiverTransplant'})\n",
    "\tdata[boolean_variables] = data[boolean_variables].replace({'Y': 1, 'N': 0})\n",
    "\tdata['Edema'] = data['Edema'].replace({'N': 'NoEdema', 'S': 'EdemaResolved', 'Y': 'EdemaPersistent'})\n",
    "\n",
    "\tglobal original_categorical_categories\n",
    "\toriginal_categorical_categories = {col: data[col].unique() for col in data.select_dtypes(include='category').columns.drop('ID')} # Guardem les categories originals de les variables categòriques\n",
    "\n",
    "\tif save_to_csv:\n",
    "\t\t# Guardem el dataset\n",
    "\t\tdata.to_csv('../assets/data/initial_preprocessing_cirrhosis.csv', index=False)\n",
    "\n",
    "#initial_preprocessing(data=data, save_to_csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Anàlisis inicial de les variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estudi de les variables numèriques\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estadístiques de les variables categòriques\n",
    "data.describe(include='category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_vars_histograms(data: pd.DataFrame):\n",
    "    # Visualització de les distribucions de les variables numèriques en una sola figura\n",
    "    numerical_columns = data.select_dtypes(include=['Int64', 'float64']).columns\n",
    "\n",
    "    num_rows = int(np.ceil(len(numerical_columns) / 2))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, num_rows * 4))\n",
    "\n",
    "    for i, col in enumerate(numerical_columns):\n",
    "        ax = fig.add_subplot(num_rows, 2, i + 1)\n",
    "        \n",
    "        sns.histplot(data[col], edgecolor=\"k\", linewidth=1.5, kde=True)\n",
    "        \n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        \n",
    "        ax.set_title(f'Distribució de la variable numèrica {col}')\n",
    "        ax.set_xlabel(col)\n",
    "        ax.set_ylabel('Freqüència')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#numerical_vars_histograms(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_vars_countplots(data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Visualització de les distribucions de les variables categòriques en una sola figura (menys ID).\n",
    "    \"\"\"\n",
    "    # Visualització de les distribucions de les variables categòriques en una sola figura (menys ID)\n",
    "    categorical_columns = data.select_dtypes(include=['category']).columns.drop(['ID'])\n",
    "    num_rows = int(np.ceil(len(categorical_columns) / 2))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, num_rows * 4))\n",
    "\n",
    "    for i, col in enumerate(categorical_columns):\n",
    "        ax = fig.add_subplot(num_rows, 2, i + 1)\n",
    "        \n",
    "        sns.countplot(data=data, x=col, ax=ax, hue=col, legend=False)\n",
    "        \n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        \n",
    "        ax.set_title(f'Distribució de la variable categòrica {col}')\n",
    "        ax.set_xlabel(col)\n",
    "        ax.set_ylabel('Quantitat')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#categorical_vars_countplots(data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tractament d'outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_iqr_factors(data: pd.DataFrame, factors: list = [1.5, 1.75, 2, 2.25, 2.5, 2.75, 3, 3.25, 3.5, 3.75, 4, 4.25, 4.5, 4.75, 5]):\n",
    "\t\"\"\"\n",
    "\tCompara diferents factors que multipliquen al IQR per a determinar els outliers i realitza un gràfic evolutiu per comparar-los.\n",
    "\t\"\"\"\n",
    "\tnumerical_columns = data.select_dtypes(include=['Int64', 'float64']).columns\n",
    "\n",
    "\tplt.figure(figsize=(10, 6))\n",
    "\n",
    "\t# Dictionary to store outlier percentages for each factor and column\n",
    "\toutlier_percentages = {col: [] for col in numerical_columns}\n",
    "\ttotal_percentages = [set() for _ in range(len(factors))]\n",
    "\n",
    "\tfor col in numerical_columns:\n",
    "\t\tQ1 = data[col].quantile(0.25)\n",
    "\t\tQ3 = data[col].quantile(0.75)\n",
    "\t\tIQR = Q3 - Q1\n",
    "\n",
    "\t\tfor f_id, factor in enumerate(factors):\n",
    "\t\t\toutliers_mask = ((data[col] < (Q1 - factor * IQR)) | (data[col] > (Q3 + factor * IQR)))\n",
    "\t\t\ttotal_percentages[f_id].update(data.index[outliers_mask])\n",
    "\t\t\toutliers_percentage = np.mean(outliers_mask) * 100\n",
    "\t\t\toutlier_percentages[col].append(outliers_percentage)\n",
    "\n",
    "\ttotal_percentages = [(len(outliers) / len(data)) * 100 for outliers in total_percentages]\n",
    "\t\t\t\n",
    "\t# Plotting the results\n",
    "\tfor col, percentages in outlier_percentages.items():\n",
    "\t\tplt.plot(factors, percentages, label=col)\n",
    "\tplt.plot(factors, total_percentages, label='Total', linestyle='--', color='black')\n",
    "\n",
    "\tplt.xlabel('Factor multiplicatiu del IQR')\n",
    "\tplt.ylabel('Percentage d\\'outliers (%)')\n",
    "\tplt.title('Percentatge d\\'outliers de cada variable numèrica per a diferents factors multiplicatius del IQR')\n",
    "\tplt.xticks(factors)\n",
    "\t\n",
    "\tplt.legend()\n",
    "\tplt.grid(True)\n",
    "\tplt.show()\n",
    "\n",
    "#compare_iqr_factors(data=data, factors=[1.5, 1.75, 2, 2.25, 2.5, 2.75, 3, 3.25, 3.5, 3.75, 4, 4.25, 4.5, 4.75, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_outliers(data: pd.DataFrame, factor: float = 1.5, plots: bool = True, save_to_csv: bool = True):\n",
    "    \"\"\"\n",
    "    Funció que detecta, visualitza i elimina els outliers d'un dataset. El factor multiplica el IQR per a determinar quins valors són outliers.\n",
    "    \"\"\"\n",
    "    # Detecció, visualització i eliminació d'outliers\n",
    "    numerical_columns = data.select_dtypes(include=['Int64', 'float64']).columns\n",
    "\n",
    "    outliers_indices = []\n",
    "\n",
    "    for col in numerical_columns:\n",
    "        Q1 = data[col].quantile(0.25)\n",
    "        Q3 = data[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - factor * IQR\n",
    "        upper_bound = Q3 + factor * IQR\n",
    "        outliers_mask = ((data[col] < lower_bound) | (data[col] > upper_bound))\n",
    "        outliers = data[col][outliers_mask]\n",
    "        non_outliers = data[col][~outliers_mask]\n",
    "\n",
    "        outliers_indices.extend(data[col][outliers_mask].index.tolist())\n",
    "        \n",
    "        if plots:\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(8, 6))\n",
    "\n",
    "            # Boxplot con los outliers originales\n",
    "            sns.boxplot(ax=axes[0, 0], y=data[col], orient='v')\n",
    "            axes[0, 0].scatter(x=[0]*len(outliers), y=outliers, color='red', marker='o')\n",
    "            axes[0, 0].set_title(f'Boxplot de {col} con outliers ({factor}x IQR)')\n",
    "\n",
    "            # Histograma con línea vertical para outliers\n",
    "            sns.histplot(ax=axes[0, 1], data=data, x=col, kde=True)\n",
    "            if (data[col] < lower_bound).any():\n",
    "                axes[0, 1].axvline(x=lower_bound, color='red', linestyle='dashed')\n",
    "            if (data[col] > upper_bound).any():\n",
    "                axes[0, 1].axvline(x=upper_bound, color='red', linestyle='dashed')\n",
    "            axes[0, 1].set_title(f'Histograma de {col}')\n",
    "            axes[0, 1].set_xlabel(col)\n",
    "            axes[0, 1].set_ylabel('Frecuencia')\n",
    "\n",
    "            # Boxplot sin los outliers\n",
    "            sns.boxplot(ax=axes[1, 0], y=non_outliers, orient='v')\n",
    "            axes[1, 0].set_title(f'Boxplot de {col} sin outliers')\n",
    "\n",
    "            # Histograma sin los outliers\n",
    "            sns.histplot(ax=axes[1, 1], data=data[~outliers_mask], x=col, kde=True)\n",
    "            axes[1, 1].set_title(f'Histograma de {col} sin outliers')\n",
    "\n",
    "            percent_outliers = len(outliers) / data.shape[0] * 100\n",
    "            fig.text(x=0.5, y=0, s=f'Outliers de {col} ({factor}x IQR): {len(outliers)} ({percent_outliers:.2f}%)', \n",
    "                    ha='center', va='center')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    unique_outliers = len(set(outliers_indices))\n",
    "\n",
    "    print(f\"Datset amb outliers: {data.shape[0]} files i {data.shape[1]} columnes.\")\n",
    "    print(f\"Nombre total d'outliers únics eliminats: {unique_outliers} ({unique_outliers / data.shape[0] * 100:.2f}% de tot el dataset).\")\n",
    "\n",
    "    # Eliminació d'outliers\n",
    "    data.drop(list(set(outliers_indices)), inplace=True)\n",
    "    \n",
    "    print(f\"Dataset sense outliers: {data.shape[0]} files i {data.shape[1]} columnes.\")\n",
    "\n",
    "    if save_to_csv:\n",
    "        # Guardem el dataset\n",
    "        data.to_csv('../assets/data/no_outliers_cirrhosis.csv', index=False)\n",
    "\n",
    "#delete_outliers(data=data, factor=3, plots=False, save_to_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Recodificació de variables categòriques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_variables(data: pd.DataFrame, save_to_csv: bool = True):\n",
    "    \"\"\"\n",
    "    Codifica les variables categòriques que calgui per a poder-les utilitzar en els models de ML. \n",
    "    A més, guarda el mapping per a poder decodificar-les.\n",
    "    Els NaNs es mantenen (en comptes de considerar-los una classe més) per poder imputar-los posteriorment.\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.impute import SimpleImputer\n",
    "\n",
    "    global ohe_mapping, original_columns_order\n",
    "\n",
    "    original_columns_order = data.columns\n",
    "\n",
    "    columns_to_encode = ['Drug', 'Sex', 'Edema', 'Stage'] # Sense la variable 'Status' perquè és la target i, a més, no té valors NaN\n",
    "\n",
    "    na_indexs_per_old_encoded_column = {col: set(data[data[col].isna()].index) for col in columns_to_encode} # Guardem els indexs dels NaNs per a cada columna a codificar\n",
    "    new_encoded_columns_per_old_encoded_column = {col: set() for col in columns_to_encode} # Guardem les classes de cada columna a codificar\n",
    "\n",
    "    # Imputem els NaNs per evitar que es crein columnes innecessàries al fer el OneHotEncoding. Després tornarem a inserir els NaNs\n",
    "    data[columns_to_encode] = SimpleImputer(strategy='most_frequent').fit_transform(data[columns_to_encode])\n",
    "\n",
    "    ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "    data_encoded = ohe.fit_transform(data[columns_to_encode])\n",
    "    encoded_columns = ohe.get_feature_names_out(columns_to_encode)\n",
    "\n",
    "    # Guardem el mapping per a poder decodificar les variables\n",
    "    ohe_mapping = {}\n",
    "    for i, col in enumerate(columns_to_encode):\n",
    "        # for category in original_categorical_categories[col]:\n",
    "        #     new_encoded_column_name = f\"{col}_{category}\"\n",
    "        #     ohe_mapping[new_encoded_column_name] = (col, category)\n",
    "        #     new_encoded_columns_per_old_encoded_column[col].add(new_encoded_column_name)\n",
    "\n",
    "        for category in ohe.categories_[i]:\n",
    "            new_encoded_column_name = f\"{col}_{category}\"\n",
    "            ohe_mapping[new_encoded_column_name] = (col, category)\n",
    "            new_encoded_columns_per_old_encoded_column[col].add(new_encoded_column_name)\n",
    "\n",
    "    data[encoded_columns] = data_encoded\n",
    "    data[encoded_columns] = data[encoded_columns].astype('category')\n",
    "\n",
    "    # Tornem a posar els NaNs per poder imputar-los\n",
    "    for col in columns_to_encode:\n",
    "        for na_index in na_indexs_per_old_encoded_column[col]:\n",
    "            for new_column in new_encoded_columns_per_old_encoded_column[col]:\n",
    "                data.loc[na_index, new_column] = np.nan\n",
    "\n",
    "    # Eliminem les columnes originals\n",
    "    data.drop(columns=columns_to_encode, inplace=True)\n",
    "\n",
    "    if save_to_csv:\n",
    "        # Guardem el dataset\n",
    "        data.to_csv('../assets/data/encoded_cirrhosis.csv', index=False)\n",
    "\n",
    "#encode_variables(data=data, save_to_csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_variables(data: pd.DataFrame, ohe_mapping, original_columns_order):\n",
    "    \"\"\"\n",
    "    Decodifica les variables categòriques que s'hagin codificat anteriorment.\n",
    "    \"\"\"\n",
    "    reconstructed_columns = {}\n",
    "\n",
    "    # Creem les columnes reconstruïdes\n",
    "    for encoded_column in ohe_mapping:\n",
    "        if encoded_column in data.columns:\n",
    "            original_column, category = ohe_mapping[encoded_column]\n",
    "\n",
    "            if original_column not in reconstructed_columns:\n",
    "                reconstructed_columns[original_column] = pd.Series([np.nan] * len(data), index=data.index, dtype='object')\n",
    "\n",
    "            category_rows = data[encoded_column] == 1\n",
    "            reconstructed_columns[original_column].loc[category_rows] = category\n",
    "\n",
    "    # Eliminem les columnes codificades\n",
    "    data.drop(columns=[col for col in ohe_mapping if col in data.columns], inplace=True)\n",
    "\n",
    "    # Inserim les columnes reconstruïdes al DataFrame\n",
    "    for col in reconstructed_columns:\n",
    "        data[col] = reconstructed_columns[col]\n",
    "        data[col] = data[col].astype(original_column_types[col])\n",
    "\n",
    "    # Reordenem les columnes perquè quedin igual que a l'original\n",
    "    data = data.reindex(columns=original_columns_order)\n",
    "\n",
    "#decode_variables(data=data, ohe_mapping=ohe_mapping, original_columns_order=original_columns_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Escalar variables numèriques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_variables(data: pd.DataFrame, scaler: str = 'standard', save_to_csv: bool = True):\n",
    "\tassert scaler in ['standard', 'minmax'], \"El paràmetre 'scaler' ha de ser 'standard' o 'minmax'.\"\n",
    "\t\"\"\"\n",
    "\tEscala les variables numèriques.\n",
    "\t\"\"\"\n",
    "\tfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\tnumerical_columns = data.select_dtypes(include=['Int64', 'float64']).columns\n",
    "\n",
    "\tsc = StandardScaler() if scaler == 'standard' else MinMaxScaler()\n",
    "\n",
    "\tdata[numerical_columns] = sc.fit_transform(data[numerical_columns])\n",
    "\n",
    "\tif save_to_csv:\n",
    "\t\t# Guardem el dataset\n",
    "\t\tdata.to_csv('../assets/data/scaled_cirrhosis.csv', index=False)\n",
    "\n",
    "# scale_variables(data=data, scaler='standard', save_to_csv=True)\n",
    "#scale_variables(data=data, scaler='minmax', save_to_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Partició del dataset en train/test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_classes_in_partitions(train: pd.DataFrame, test: pd.DataFrame):\n",
    "\t\"\"\"\n",
    "\tCertifica que tant el conjunt d'entrenament com el de prova continguin almenys un exemple de cada classe de totes les variables categòriques (excepte 'ID').\n",
    "\tEn cas que no hi hagi cap exemple d'una classe en un dels dos conjunts, es mou una mostra del conjunt que en tingui a l'altre.\n",
    "\tAixò evita problemes en cas que es faci encoding.\n",
    "\t\"\"\"\n",
    "\tcategorical_cols = data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\tcategorical_cols.remove('ID')  # Excluir 'ID'\n",
    "\n",
    "\t# Comprovar que tant el conjunt d'entrenament com el de prova continguin almenys un exemple de cada classe\n",
    "\tfor col in categorical_cols:\n",
    "\t\ttrain_classes = train[col].notna().unique()\n",
    "\t\ttest_classes = test[col].notna().unique()\n",
    "\n",
    "\t\tmissing_classes_test = set(train_classes) - set(test_classes)\n",
    "\t\tmissing_classes_train = set(test_classes) - set(train_classes)\n",
    "\t\t\n",
    "\t\tfor missing_class in missing_classes_test:\n",
    "\t\t\tprint(f\"Missing class '{missing_class}' in test set for column '{col}'\")\n",
    "\t\t\t# Moure una mostra amb la classe faltant del conjunt de entrenament al de prova\n",
    "\t\t\tmissing_class_index = train[train[col] == missing_class].index[0]\n",
    "\t\t\ttest = pd.concat([test, train.loc[[missing_class_index]]])\n",
    "\t\t\ttrain.drop(missing_class_index, inplace=True)\n",
    "\n",
    "\t\tfor missing_class in missing_classes_train:\n",
    "\t\t\tprint(f\"Missing class '{missing_class}' in train set for column '{col}'\")\n",
    "\t\t\t# Moure una mostra amb la classe faltant del conjunt de prova al de entrenament\n",
    "\t\t\tmissing_class_index = test[test[col] == missing_class].index[0]\n",
    "\t\t\ttrain = pd.concat([train, test.loc[[missing_class_index]]])\n",
    "\t\t\ttest.drop(missing_class_index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data: pd.DataFrame, test_size: float = 0.15, stratify: bool = True, random_state: int = 42):\n",
    "\t\"\"\"\n",
    "\tParticiona el dataset en conjunts de entrenament i prova.\n",
    "\tAssegura que tant el conjunt d'entrenament com el de prova continguin almenys un exemple de cada classe\n",
    "\tde les variables categòriques (excepte 'ID') per evitar problemes en cas que es faci encoding.\n",
    "\t\"\"\"\n",
    "\tglobal train, test, X_train, y_train, X_test, y_test\n",
    "\t\n",
    "\tfrom sklearn.model_selection import train_test_split\n",
    "\n",
    "\t# Particionem el dataset en conjunts de entrenament i prova\n",
    "\tif stratify:\n",
    "\t\ttrain, test = train_test_split(data, test_size=test_size, random_state=random_state, stratify=data['Status'])\n",
    "\telse:\n",
    "\t\ttrain, test = train_test_split(data, test_size=test_size, random_state=random_state)\n",
    "\n",
    "\t# Comprovem que tant el conjunt d'entrenament com el de prova continguin almenys un exemple de cada classe \n",
    "\t# de totes les variables categòriques (excepte 'ID')\n",
    "\tkeep_classes_in_partitions(train=train, test=test)\n",
    "\n",
    "\t# 'Status' es la variable target\n",
    "\tX_train = train.drop(columns=['Status'])\n",
    "\ty_train = train['Status']\n",
    "\tX_test = test.drop(columns=['Status'])\n",
    "\ty_test = test['Status']\n",
    "\n",
    "\tprint(f\"Train shape: {train.shape}\")\n",
    "\tprint(f\"Test shape: {test.shape}\")\n",
    "\n",
    "#split_dataset(data=data, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imputar els valors faltants (Missings)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimeix les variables que tenen valors NaN, el seu percentatge i el seu tipus de dades\n",
    "for col_train in train.columns:\n",
    "\tif train[col_train].isna().any():\n",
    "\t\tprint(f\"{col_train}: {train[col_train].isna().sum()} NaNs ({train[col_train].isna().sum() / len(train) * 100:.2f}%) ({train[col_train].dtype})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_imputer(data_is_encoded: bool, \\\n",
    "                 X_train: pd.DataFrame, \\\n",
    "                    random_state: int = 42, \\\n",
    "                        print_scores: bool = True, \\\n",
    "                            proportion_to_test_imputation: float = 0.1, \\\n",
    "                                num_metric = 'r2', \\\n",
    "                                    cat_metric = 'accuracy'):\n",
    "    \"\"\"\n",
    "    Prova diferents imputadors, imprimeix els seus resultats i retorna el millor.\n",
    "    \"\"\"\n",
    "    assert proportion_to_test_imputation > 0 and proportion_to_test_imputation < 1, \"El paràmetre 'proportion_to_test_imputation' ha de ser un valor entre 0 i 1.\"\n",
    "    num_metric_values = {'r2'}\n",
    "    assert num_metric in num_metric_values, f\"El paràmetre 'num_metric' ha de ser un dels següents: {num_metric_values}.\"\n",
    "    cat_metric_values = {'accuracy', 'f1', 'precision', 'recall'}\n",
    "    assert cat_metric in cat_metric_values, f\"El paràmetre 'cat_metric' ha de ser un dels següents: {cat_metric_values}.\"\n",
    "    \n",
    "    from sklearn.impute import KNNImputer, SimpleImputer\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.metrics import r2_score, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "    metrics_mapping = {'r2': r2_score, 'accuracy': accuracy_score, 'f1': f1_score, 'precision': precision_score, 'recall': recall_score}\n",
    "\n",
    "    num_metric = metrics_mapping[num_metric]\n",
    "    cat_metric = metrics_mapping[cat_metric]\n",
    "\n",
    "    numerical_columns = X_train.select_dtypes(include=['Int64', 'float64']).columns\n",
    "    categorical_columns = X_train.select_dtypes(include=['category']).columns.drop(['ID'])\n",
    "    original_cols_with_na = X_train.columns[X_train.isna().any()]\n",
    "\n",
    "    MixedImputer = ColumnTransformer([\n",
    "        ('numerical', SimpleImputer(strategy='mean'), numerical_columns),\n",
    "        ('categorical', SimpleImputer(strategy='most_frequent'), categorical_columns)\n",
    "    ])\n",
    "\n",
    "    imputers: dict = {'mixed': MixedImputer}\n",
    "\n",
    "    if data_is_encoded:\n",
    "        imputers['knn-1'] = KNNImputer(n_neighbors=1)\n",
    "        imputers['knn-3'] = KNNImputer(n_neighbors=3)\n",
    "        imputers['knn-5'] = KNNImputer(n_neighbors=5)\n",
    "\n",
    "    # from sklearn.experimental import enable_iterative_imputer\n",
    "    # from sklearn.impute import IterativeImputer\n",
    "    # imputers['iterative-10'] = IterativeImputer(max_iter=10, random_state=random_state)\n",
    "    # imputers['iterative-20'] = IterativeImputer(max_iter=20, random_state=random_state)\n",
    "\n",
    "    scores = {}\n",
    "    best_score = float('-inf')\n",
    "    best_imputer = (None, None)\n",
    "    best_imputer_name = 'None'\n",
    "\n",
    "    # Ens quedem només amb les files sense NaNs i sense la variable 'ID' (ja que no aporta informació)\n",
    "    X_train_complete = X_train.dropna().drop(columns=['ID'])\n",
    "\n",
    "    # Creem un dataset amb NaNs aleatoris per imputar\n",
    "    X_train_incomplete = X_train_complete.copy()\n",
    "    for col in original_cols_with_na:\n",
    "        X_train_incomplete.loc[X_train_incomplete.sample(frac=proportion_to_test_imputation, random_state=random_state).index, col] = np.nan\n",
    "\n",
    "    # Imputar i calcular mètriques\n",
    "    for name_imputer, imputer in imputers.items():\n",
    "        # Imputar\n",
    "        imputed_data = imputer.fit_transform(X_train_incomplete)\n",
    "\n",
    "        # Convertir a DataFrame y asegurarse de que las columnas coincidan\n",
    "        if isinstance(imputer, ColumnTransformer):\n",
    "            # Extraer los nombres de las columnas después de la transformación\n",
    "            transformed_columns = [col for name, trans, cols in imputer.transformers if trans != 'drop' for col in cols]\n",
    "            X_train_imputed = pd.DataFrame(imputed_data, columns=transformed_columns, index=X_train_incomplete.index)\n",
    "        else:\n",
    "            # Para otros imputadores, simplemente usa las columnas originales\n",
    "            X_train_imputed = pd.DataFrame(imputed_data, columns=X_train_incomplete.columns, index=X_train_incomplete.index)\n",
    "\n",
    "        # Calcular mètriques\n",
    "        num_scores = {} # Per a les variables numèriques\n",
    "        cat_scores = {} # Per a les variables categòriques\n",
    "        for col in original_cols_with_na:\n",
    "            if col in numerical_columns:\n",
    "                num_score = num_metric(X_train_complete[col], X_train_imputed[col])\n",
    "                num_scores[col] = num_score\n",
    "            elif col in categorical_columns:\n",
    "                cat_score = cat_metric(X_train_complete[col].astype('category'), np.round(X_train_imputed[col].astype('float64')).astype('category'))\n",
    "                cat_scores[col] = cat_score\n",
    "\n",
    "        overall_score = np.mean(list(num_scores.values()) + list(cat_scores.values()))\n",
    "        scores[name_imputer] = {'categorical': cat_scores, 'numerical': num_scores, 'overall': overall_score}\n",
    "\n",
    "        # Guardar el millor imputador\n",
    "        if overall_score > best_score:\n",
    "            best_score = overall_score\n",
    "            best_imputer = imputer\n",
    "            best_imputer_name = name_imputer\n",
    "\n",
    "    # Imprimir els resultats\n",
    "    if print_scores:\n",
    "        for name_imputer, scores_imputer in scores.items():\n",
    "            print(f\"IMPUTER [{name_imputer}]: {scores_imputer['overall']} (overall score)\")\n",
    "            print(f\"\\t*Variables numèriques ({num_metric}): {np.mean(list(scores_imputer['numerical'].values()))}\")\n",
    "            for col, s in scores_imputer['numerical'].items():\n",
    "                print(f\"\\t\\t*{col}: {s}\")\n",
    "            print(f\"\\t*Variables categòriques ({cat_metric}): {np.mean(list(scores_imputer['categorical'].values()))}\")\n",
    "            for col, s in scores_imputer['categorical'].items():\n",
    "                print(f\"\\t\\t*{col}: {s}\")\n",
    "            print()\n",
    "\n",
    "        print(f\"MILLOR IMPUTER OVERALL --> {best_imputer_name} ({best_score})\")\n",
    "    \n",
    "    # Retornar el millor imputador\n",
    "    return best_imputer_name, best_imputer, best_score\n",
    "\n",
    "#best_imputer(X_train=X_train, random_state=42, print_scores=True, return_best_imputer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_data(data_to_impute: pd.DataFrame, \\\n",
    "\t\t\t\timputer = 'best', \\\n",
    "\t\t\t\t\tsave_to_csv: bool = True, \\\n",
    "\t\t\t\t\t\trandom_state: int = 42, \\\n",
    "\t\t\t\t\t\t\tencode: bool = True, \\\n",
    "\t\t\t\t\t\t\t\tdecode: bool = True, \\\n",
    "\t\t\t\t\t\t\t\t\tproportion_to_test_imputation: float = 0.1, \\\n",
    "\t\t\t\t\t\t\t\t\t\tnum_metric = 'r2', \\\n",
    "\t\t\t\t\t\t\t\t\t\t\tcat_metric = 'accuracy'):\n",
    "\t\"\"\"\n",
    "\tImputa els valors NaN del dataset.\n",
    "\t\"\"\"\n",
    "\tfrom sklearn.compose import ColumnTransformer\n",
    "\n",
    "\t# Si no hi ha cap NaN, no cal imputar\n",
    "\tif not data_to_impute.isna().values.any():\n",
    "\t\tprint(\"No hi ha cap NaN al dataset.\")\n",
    "\t\treturn\n",
    "\n",
    "\tif imputer == 'best':\n",
    "\t\tif encode:\n",
    "\t\t\t# Codifiquem les variables categòriques\n",
    "\t\t\tencode_variables(data=data_to_impute, save_to_csv=False)\n",
    "\n",
    "\t\tname_imputer, imputer, score_imputer = best_imputer(data_is_encoded=encode, \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t  X_train=X_train, \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\trandom_state=random_state, \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tprint_scores=False, \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tproportion_to_test_imputation=proportion_to_test_imputation, \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnum_metric=num_metric, \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcat_metric=cat_metric)\n",
    "\n",
    "\t\tprint(f\"IMPUTADOR SELECCIONAT: {name_imputer} ({score_imputer} overall score imputant en X_train)\")\n",
    "\t\n",
    "\timputed_data = imputer.fit_transform(data_to_impute)\n",
    "\n",
    "\tif isinstance(imputer, ColumnTransformer):\n",
    "\t\t# Extraer los nombres de las columnas después de la transformación\n",
    "\t\ttransformed_columns = [col for name, trans, cols in imputer.transformers if trans != 'drop' for col in cols]\n",
    "\t\tdata_to_impute = pd.DataFrame(imputed_data, columns=transformed_columns, index=data_to_impute.index)\n",
    "\telse:\n",
    "\t\t# Para otros imputadores, simplemente usa las columnas originales\n",
    "\t\tdata_to_impute = pd.DataFrame(imputed_data, columns=data_to_impute.columns, index=data_to_impute.index)\n",
    "\n",
    "\t# Comprovem que ja no hi hagi NaNs\n",
    "\tif data_to_impute.isna().values.any():\n",
    "\t\traise Exception(\"Per algun motiu desconegut, encara hi ha NaNs al dataset imputat.\")\n",
    "\t\n",
    "\tif decode:\n",
    "\t\t# Decodifiquem les variables categòriques\n",
    "\t\tdecode_variables(data=data_to_impute, ohe_mapping=ohe_mapping, original_columns_order=original_columns_order)\n",
    "\t\n",
    "\tif save_to_csv:\n",
    "\t\t# Guardem el dataset\n",
    "\t\tdata_to_impute.to_csv('../assets/data/imputed_cirrhosis.csv', index=False)\n",
    "\n",
    "#impute_data(data_to_impute=X_train, imputer='best', save_to_csv=True, random_state=42, encode=True, decode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random state: 687\n",
      "Datset amb outliers: 418 files i 20 columnes.\n",
      "Nombre total d'outliers únics eliminats: 70 (16.75% de tot el dataset).\n",
      "Dataset sense outliers: 348 files i 20 columnes.\n",
      "Train shape: (295, 20)\n",
      "Test shape: (53, 20)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Status'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Cai Selvas Sala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3789\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3790\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3791\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Status'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Cai Selvas Sala\\GIA_UPC\\2nC\\1rQ\\IAA\\Practica\\src\\main.ipynb Cell 38\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y100sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m scale_variables(data\u001b[39m=\u001b[39mdata, scaler\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mminmax\u001b[39m\u001b[39m'\u001b[39m, save_to_csv\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y100sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m split_dataset(data\u001b[39m=\u001b[39mdata, test_size\u001b[39m=\u001b[39m\u001b[39m0.15\u001b[39m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y100sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m impute_data(data_to_impute\u001b[39m=\u001b[39;49mX_train, imputer\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mbest\u001b[39;49m\u001b[39m'\u001b[39;49m, save_to_csv\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, random_state\u001b[39m=\u001b[39;49mrandom_state, encode\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, decode\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, proportion_to_test_imputation\u001b[39m=\u001b[39;49m\u001b[39m0.15\u001b[39;49m, num_metric\u001b[39m=\u001b[39;49mnum_metric, cat_metric\u001b[39m=\u001b[39;49mcat_metric)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y100sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m impute_data(data_to_impute\u001b[39m=\u001b[39mX_test, imputer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbest\u001b[39m\u001b[39m'\u001b[39m, save_to_csv\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state, encode\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, decode\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, proportion_to_test_imputation\u001b[39m=\u001b[39m\u001b[39m0.15\u001b[39m, num_metric\u001b[39m=\u001b[39mnum_metric, cat_metric\u001b[39m=\u001b[39mcat_metric)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y100sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m decode_variables(data\u001b[39m=\u001b[39mX_train, ohe_mapping\u001b[39m=\u001b[39mohe_mapping, original_columns_order\u001b[39m=\u001b[39moriginal_columns_order)\n",
      "\u001b[1;32mc:\\Users\\Cai Selvas Sala\\GIA_UPC\\2nC\\1rQ\\IAA\\Practica\\src\\main.ipynb Cell 38\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y100sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \t\u001b[39mif\u001b[39;00m encode:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y100sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \t\t\u001b[39m# Codifiquem les variables categòriques\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y100sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \t\tencode_variables(data\u001b[39m=\u001b[39mdata_to_impute, save_to_csv\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y100sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \tname_imputer, imputer, score_imputer \u001b[39m=\u001b[39m best_imputer(data_is_encoded\u001b[39m=\u001b[39;49mencode, \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y100sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \t\t\t\t\t\t\t\t\t\t\t\t  X_train\u001b[39m=\u001b[39;49mX_train, \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y100sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \t\t\t\t\t\t\t\t\t\t\t\t\trandom_state\u001b[39m=\u001b[39;49mrandom_state, \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y100sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \t\t\t\t\t\t\t\t\t\t\t\t\t\tprint_scores\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y100sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \t\t\t\t\t\t\t\t\t\t\t\t\t\t\tproportion_to_test_imputation\u001b[39m=\u001b[39;49mproportion_to_test_imputation, \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y100sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnum_metric\u001b[39m=\u001b[39;49mnum_metric, \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y100sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcat_metric\u001b[39m=\u001b[39;49mcat_metric)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y100sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \t\u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIMPUTADOR SELECCIONAT: \u001b[39m\u001b[39m{\u001b[39;00mname_imputer\u001b[39m}\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m{\u001b[39;00mscore_imputer\u001b[39m}\u001b[39;00m\u001b[39m overall score imputant en X_train)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y100sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m imputed_data \u001b[39m=\u001b[39m imputer\u001b[39m.\u001b[39mfit_transform(data_to_impute)\n",
      "\u001b[1;32mc:\\Users\\Cai Selvas Sala\\GIA_UPC\\2nC\\1rQ\\IAA\\Practica\\src\\main.ipynb Cell 38\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y100sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m original_cols_with_na:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y100sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     X_train_incomplete\u001b[39m.\u001b[39mloc[X_train_incomplete\u001b[39m.\u001b[39msample(frac\u001b[39m=\u001b[39mproportion_to_test_imputation, random_state\u001b[39m=\u001b[39mrandom_state)\u001b[39m.\u001b[39mindex, col] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnan\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y100sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m keep_classes_in_partitions(train\u001b[39m=\u001b[39;49mX_train_complete, test\u001b[39m=\u001b[39;49mX_train_incomplete)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y100sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39m# Imputar i calcular mètriques\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y100sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39mfor\u001b[39;00m name_imputer, imputer \u001b[39min\u001b[39;00m imputers\u001b[39m.\u001b[39mitems():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y100sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     \u001b[39m# Imputar\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\Cai Selvas Sala\\GIA_UPC\\2nC\\1rQ\\IAA\\Practica\\src\\main.ipynb Cell 38\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y100sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Comprovar que tant el conjunt d'entrenament com el de prova continguin almenys un exemple de cada classe\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y100sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m categorical_cols:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y100sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \ttrain_classes \u001b[39m=\u001b[39m train[col]\u001b[39m.\u001b[39mnotna()\u001b[39m.\u001b[39munique()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y100sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \ttest_classes \u001b[39m=\u001b[39m test[col]\u001b[39m.\u001b[39mnotna()\u001b[39m.\u001b[39munique()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y100sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \tmissing_classes_test \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(train_classes) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m(test_classes)\n",
      "File \u001b[1;32mc:\\Users\\Cai Selvas Sala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:3896\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3894\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3895\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3896\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3897\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3898\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\Cai Selvas Sala\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3792\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(casted_key, \u001b[39mslice\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[0;32m   3793\u001b[0m         \u001b[39misinstance\u001b[39m(casted_key, abc\u001b[39m.\u001b[39mIterable)\n\u001b[0;32m   3794\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(x, \u001b[39mslice\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m casted_key)\n\u001b[0;32m   3795\u001b[0m     ):\n\u001b[0;32m   3796\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3797\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3798\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3799\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3800\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Status'"
     ]
    }
   ],
   "source": [
    "# Pipeline per realizar tot un experiment\n",
    "import random\n",
    "random_state = 687\n",
    "print(f\"Random state: {random_state}\")\n",
    "num_metric = 'r2'\n",
    "cat_metric = 'precision'\n",
    "load_dataset(save_to_csv=False)\n",
    "initial_preprocessing(data=data, save_to_csv=False)\n",
    "delete_outliers(data=data, factor=3, plots=False, save_to_csv=False)\n",
    "#encode_variables(data=data, save_to_csv=False)\n",
    "scale_variables(data=data, scaler='minmax', save_to_csv=False)\n",
    "split_dataset(data=data, test_size=0.15, random_state=random_state)\n",
    "impute_data(data_to_impute=X_train, imputer='best', save_to_csv=False, random_state=random_state, encode=True, decode=True, proportion_to_test_imputation=0.15, num_metric=num_metric, cat_metric=cat_metric)\n",
    "impute_data(data_to_impute=X_test, imputer='best', save_to_csv=False, random_state=random_state, encode=True, decode=True, proportion_to_test_imputation=0.15, num_metric=num_metric, cat_metric=cat_metric)\n",
    "decode_variables(data=X_train, ohe_mapping=ohe_mapping, original_columns_order=original_columns_order)\n",
    "decode_variables(data=X_test, ohe_mapping=ohe_mapping, original_columns_order=original_columns_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_con_imputacion(X, y):\n",
    "    from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "    kf = KFold(n_splits=5)  # Ajustar según necesidades\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        imputacion = mejor_imputacion(X_train, y_train)[0][1]\n",
    "        # Aplicar la mejor imputación y entrenar el modelo\n",
    "        \n",
    "        X_train_imputed = imputacion.fit_transform(X_train)\n",
    "\n",
    "        # Evaluar el modelo en X_test, y_test..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Correlacions entre variables numèriques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_vars_correlations(data: pd.DataFrame):\n",
    "\t\"\"\"\n",
    "\tVisualitza la correlació entre les variables numèriques.\n",
    "\t\"\"\"\n",
    "\n",
    "\tnumerical_columns = data.select_dtypes(include=['Int64', 'float64']).columns\n",
    "\n",
    "\tplt.figure(figsize=(10, 6))\n",
    "\n",
    "\tsns.heatmap(data[numerical_columns].corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "\n",
    "\tplt.title('Correlació entre les variables numèriques')\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()\n",
    "\n",
    "\n",
    "numerical_vars_correlations(data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1r Model: K-Nearest Neighbors (KNN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2n Model: Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3r Model: Support Vector Machine (SVM)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
