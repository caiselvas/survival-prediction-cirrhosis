{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IAA - PRÀCTICA: MAIN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Instal·lar llibreries necessàries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../assets/requirements.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importar llibreries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_dependencies():\n",
    "\tglobal pd, np, plt, sns, skl\n",
    "\n",
    "\timport pandas as pd\n",
    "\timport numpy as np\n",
    "\timport matplotlib.pyplot as plt\n",
    "\timport seaborn as sns\n",
    "\timport sklearn as skl\n",
    "\n",
    "import_dependencies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Llegir les dades (Cirrhosis Dataset)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(save_to_csv: bool = True):\n",
    "\tglobal data\n",
    "\tfrom ucimlrepo import fetch_ucirepo \n",
    "\t\n",
    "\t# Fetch dataset\n",
    "\tcirrhosis_patient_survival_prediction = fetch_ucirepo(id=878)\n",
    "\n",
    "\tdata = pd.DataFrame(cirrhosis_patient_survival_prediction.data.original)\n",
    "\n",
    "\tif save_to_csv:\n",
    "\t\t# Guardem el dataset per poder-lo visualitzar sencer\n",
    "\t\tdata.to_csv('../assets/data/raw_cirrhosis.csv', index=False)\n",
    "\n",
    "#load_dataset(save_to_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Informació del dataset inicial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preprocessing inicial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_preprocessing(data: pd.DataFrame, save_to_csv: bool = True):\n",
    "\t\"\"\"\n",
    "\tReemplaça els valors 'NaNN' per NaN, assigna els tipus de dades correctes a cada columna i renombra les classes d'algunes variables per una millor comprensió.\n",
    "\t\"\"\"\n",
    "\t# Reemplaçar l'string 'NaNN' per NaN\n",
    "\tdata.replace(to_replace=['NaNN', '', pd.NA], value=np.nan, inplace=True)\n",
    "\n",
    "\t# Eliminem variables que no aporten informació\n",
    "\tdata.drop(columns=['ID'], inplace=True)\n",
    "\n",
    "\t# Assignem els tipus de dades correctes a cada columna\n",
    "\tint64_variables = ['N_Days', 'Age', 'Cholesterol', 'Copper', 'Tryglicerides', 'Platelets']\n",
    "\tfloat64_variables = ['Bilirubin', 'Albumin', 'Alk_Phos', 'SGOT', 'Prothrombin']\n",
    "\tcategory_variables = ['Status', 'Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema', 'Stage']\n",
    "\tboolean_variables = ['Ascites', 'Hepatomegaly', 'Spiders']\n",
    "\n",
    "\tdata[int64_variables] = data[int64_variables].astype('Int64')\n",
    "\tdata[float64_variables] = data[float64_variables].astype('float64')\n",
    "\tdata[category_variables] = data[category_variables].astype('category')\n",
    "\n",
    "\tglobal original_column_types\n",
    "\n",
    "\toriginal_column_types = {col: str(data[col].dtype) for col in data.columns} # Guardem els tipus de dades de cada columna\n",
    "\n",
    "\t# Renombrem les classes d'algunes variables per una millor comprensió\n",
    "\tdata['Status'] = data['Status'].replace({'D': 'Dead', 'C': 'Alive', 'CL': 'LiverTransplant'})\n",
    "\tdata[boolean_variables] = data[boolean_variables].replace({'Y': 1, 'N': 0})\n",
    "\tdata['Edema'] = data['Edema'].replace({'N': 'NoEdema', 'S': 'EdemaResolved', 'Y': 'EdemaPersistent'})\n",
    "\n",
    "\tglobal original_categorical_categories\n",
    "\toriginal_categorical_categories = {col: data[col].unique() for col in data.select_dtypes(include='category').columns} # Guardem les categories originals de les variables categòriques\n",
    "\n",
    "\tif save_to_csv:\n",
    "\t\t# Guardem el dataset\n",
    "\t\tdata.to_csv('../assets/data/initial_preprocessing_cirrhosis.csv', index=False)\n",
    "\n",
    "#initial_preprocessing(data=data, save_to_csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Anàlisis inicial de les variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estudi de les variables numèriques\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estadístiques de les variables categòriques\n",
    "data.describe(include='category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_vars_histograms(data: pd.DataFrame):\n",
    "    # Visualització de les distribucions de les variables numèriques en una sola figura\n",
    "    numerical_columns = data.select_dtypes(include=['Int64', 'float64']).columns\n",
    "\n",
    "    num_rows = int(np.ceil(len(numerical_columns) / 2))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, num_rows * 4))\n",
    "\n",
    "    for i, col in enumerate(numerical_columns):\n",
    "        ax = fig.add_subplot(num_rows, 2, i + 1)\n",
    "        \n",
    "        sns.histplot(data[col], edgecolor=\"k\", linewidth=1.5, kde=True)\n",
    "        \n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        \n",
    "        ax.set_title(f'Distribució de la variable numèrica {col}')\n",
    "        ax.set_xlabel(col)\n",
    "        ax.set_ylabel('Freqüència')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#numerical_vars_histograms(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_vars_countplots(data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Visualització de les distribucions de les variables categòriques en una sola figura.\n",
    "    \"\"\"\n",
    "    # Visualització de les distribucions de les variables categòriques en una sola figura\n",
    "    categorical_columns = data.select_dtypes(include=['category']).columns\n",
    "    num_rows = int(np.ceil(len(categorical_columns) / 2))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, num_rows * 4))\n",
    "\n",
    "    for i, col in enumerate(categorical_columns):\n",
    "        ax = fig.add_subplot(num_rows, 2, i + 1)\n",
    "        \n",
    "        sns.countplot(data=data, x=col, ax=ax, hue=col, legend=False)\n",
    "        \n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        \n",
    "        ax.set_title(f'Distribució de la variable categòrica {col}')\n",
    "        ax.set_xlabel(col)\n",
    "        ax.set_ylabel('Quantitat')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#categorical_vars_countplots(data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tractament d'outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_iqr_factors(data: pd.DataFrame, factors: list = [1.5, 1.75, 2, 2.25, 2.5, 2.75, 3, 3.25, 3.5, 3.75, 4, 4.25, 4.5, 4.75, 5]):\n",
    "\t\"\"\"\n",
    "\tCompara diferents factors que multipliquen al IQR per a determinar els outliers i realitza un gràfic evolutiu per comparar-los.\n",
    "\t\"\"\"\n",
    "\tnumerical_columns = data.select_dtypes(include=['Int64', 'float64']).columns\n",
    "\n",
    "\tplt.figure(figsize=(10, 6))\n",
    "\n",
    "\t# Dictionary to store outlier percentages for each factor and column\n",
    "\toutlier_percentages = {col: [] for col in numerical_columns}\n",
    "\ttotal_percentages = [set() for _ in range(len(factors))]\n",
    "\n",
    "\tfor col in numerical_columns:\n",
    "\t\tQ1 = data[col].quantile(0.25)\n",
    "\t\tQ3 = data[col].quantile(0.75)\n",
    "\t\tIQR = Q3 - Q1\n",
    "\n",
    "\t\tfor f_id, factor in enumerate(factors):\n",
    "\t\t\toutliers_mask = ((data[col] < (Q1 - factor * IQR)) | (data[col] > (Q3 + factor * IQR)))\n",
    "\t\t\ttotal_percentages[f_id].update(data.index[outliers_mask])\n",
    "\t\t\toutliers_percentage = np.mean(outliers_mask) * 100\n",
    "\t\t\toutlier_percentages[col].append(outliers_percentage)\n",
    "\n",
    "\ttotal_percentages = [(len(outliers) / len(data)) * 100 for outliers in total_percentages]\n",
    "\t\t\t\n",
    "\t# Plotting the results\n",
    "\tfor col, percentages in outlier_percentages.items():\n",
    "\t\tplt.plot(factors, percentages, label=col)\n",
    "\tplt.plot(factors, total_percentages, label='Total', linestyle='--', color='black')\n",
    "\n",
    "\tplt.xlabel('Factor multiplicatiu del IQR')\n",
    "\tplt.ylabel('Percentage d\\'outliers (%)')\n",
    "\tplt.title('Percentatge d\\'outliers de cada variable numèrica per a diferents factors multiplicatius del IQR')\n",
    "\tplt.xticks(factors)\n",
    "\t\n",
    "\tplt.legend()\n",
    "\tplt.grid(True)\n",
    "\tplt.show()\n",
    "\n",
    "#compare_iqr_factors(data=data, factors=[1.5, 1.75, 2, 2.25, 2.5, 2.75, 3, 3.25, 3.5, 3.75, 4, 4.25, 4.5, 4.75, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_outliers(data: pd.DataFrame, factor: float = 1.5, plots: bool = True, save_to_csv: bool = True):\n",
    "    \"\"\"\n",
    "    Funció que detecta, visualitza i elimina els outliers d'un dataset. El factor multiplica el IQR per a determinar quins valors són outliers.\n",
    "    \"\"\"\n",
    "    # Detecció, visualització i eliminació d'outliers\n",
    "    numerical_columns = data.select_dtypes(include=['Int64', 'float64']).columns\n",
    "\n",
    "    outliers_indices = []\n",
    "\n",
    "    for col in numerical_columns:\n",
    "        Q1 = data[col].quantile(0.25)\n",
    "        Q3 = data[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - factor * IQR\n",
    "        upper_bound = Q3 + factor * IQR\n",
    "        outliers_mask = ((data[col] < lower_bound) | (data[col] > upper_bound))\n",
    "        outliers = data[col][outliers_mask]\n",
    "        non_outliers = data[col][~outliers_mask]\n",
    "\n",
    "        outliers_indices.extend(data[col][outliers_mask].index.tolist())\n",
    "        \n",
    "        if plots:\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(8, 6))\n",
    "\n",
    "            # Boxplot con los outliers originales\n",
    "            sns.boxplot(ax=axes[0, 0], y=data[col], orient='v')\n",
    "            axes[0, 0].scatter(x=[0]*len(outliers), y=outliers, color='red', marker='o')\n",
    "            axes[0, 0].set_title(f'Boxplot de {col} con outliers ({factor}x IQR)')\n",
    "\n",
    "            # Histograma con línea vertical para outliers\n",
    "            sns.histplot(ax=axes[0, 1], data=data, x=col, kde=True)\n",
    "            if (data[col] < lower_bound).any():\n",
    "                axes[0, 1].axvline(x=lower_bound, color='red', linestyle='dashed')\n",
    "            if (data[col] > upper_bound).any():\n",
    "                axes[0, 1].axvline(x=upper_bound, color='red', linestyle='dashed')\n",
    "            axes[0, 1].set_title(f'Histograma de {col}')\n",
    "            axes[0, 1].set_xlabel(col)\n",
    "            axes[0, 1].set_ylabel('Frecuencia')\n",
    "\n",
    "            # Boxplot sin los outliers\n",
    "            sns.boxplot(ax=axes[1, 0], y=non_outliers, orient='v')\n",
    "            axes[1, 0].set_title(f'Boxplot de {col} sin outliers')\n",
    "\n",
    "            # Histograma sin los outliers\n",
    "            sns.histplot(ax=axes[1, 1], data=data[~outliers_mask], x=col, kde=True)\n",
    "            axes[1, 1].set_title(f'Histograma de {col} sin outliers')\n",
    "\n",
    "            percent_outliers = len(outliers) / data.shape[0] * 100\n",
    "            fig.text(x=0.5, y=0, s=f'Outliers de {col} ({factor}x IQR): {len(outliers)} ({percent_outliers:.2f}%)', \n",
    "                    ha='center', va='center')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    unique_outliers = len(set(outliers_indices))\n",
    "\n",
    "    print(f\"Datset amb outliers: {data.shape[0]} files i {data.shape[1]} columnes.\")\n",
    "    print(f\"Nombre total d'outliers únics eliminats: {unique_outliers} ({unique_outliers / data.shape[0] * 100:.2f}% de tot el dataset).\")\n",
    "\n",
    "    # Eliminació d'outliers\n",
    "    data.drop(list(set(outliers_indices)), inplace=True)\n",
    "\n",
    "    \n",
    "    print(f\"Dataset sense outliers: {data.shape[0]} files i {data.shape[1]} columnes.\")\n",
    "\n",
    "    if save_to_csv:\n",
    "        # Guardem el dataset\n",
    "        data.to_csv('../assets/data/no_outliers_cirrhosis.csv', index=False)\n",
    "\n",
    "#delete_outliers(data=data, factor=3, plots=False, save_to_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Funció per tractar les últimes files del dataset (amb molts missings)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_last_rows(data: pd.DataFrame, save_to_csv: bool = True):\n",
    "\t\"\"\"\n",
    "\tElimina les últimes files del dataset (les que contenen 9 NaNs).\n",
    "\t\"\"\"\n",
    "\t# Eliminem les últimes files del dataset (les que contenen 9 NaNs)\n",
    "\tdata.drop(data[data.isna().sum(axis=1) == 9].index, inplace=True)\n",
    "\n",
    "\tif save_to_csv:\n",
    "\t\t# Guardem el dataset\n",
    "\t\tdata.to_csv('../assets/data/last_rows_deleted_cirrhosis.csv', index=False)\n",
    "\n",
    "#delete_last_rows(data=data, save_to_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Recodificació de variables categòriques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_variables(data: pd.DataFrame, save_to_csv: bool = True):\n",
    "    \"\"\"\n",
    "    Codifica les variables categòriques que calgui per a poder-les utilitzar en els models de ML. \n",
    "    A més, guarda el mapping per a poder decodificar-les.\n",
    "    \"\"\"\n",
    "    assert not data.isna().any().any(), \"Per codificar el dataset cal que no hi hagi valors NaN.\"\n",
    "\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "    global ohe_mapping, original_columns_order\n",
    "\n",
    "    original_columns_order = data.columns\n",
    "\n",
    "    columns_to_encode = ['Drug', 'Sex', 'Edema', 'Stage'] # Sense la variable 'Status' perquè és la target i, a més, no té valors NaN\n",
    "\n",
    "    new_encoded_columns_per_old_encoded_column = {col: set() for col in columns_to_encode} # Guardem les classes de cada columna a codificar\n",
    "\n",
    "    ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "    data_encoded = ohe.fit_transform(data[columns_to_encode])\n",
    "    encoded_columns = ohe.get_feature_names_out(columns_to_encode)\n",
    "\n",
    "    # Guardem el mapping per a poder decodificar les variables\n",
    "    ohe_mapping = {}\n",
    "    for i, col in enumerate(columns_to_encode):\n",
    "        # for category in original_categorical_categories[col]:\n",
    "        #     new_encoded_column_name = f\"{col}_{category}\"\n",
    "        #     ohe_mapping[new_encoded_column_name] = (col, category)\n",
    "        #     new_encoded_columns_per_old_encoded_column[col].add(new_encoded_column_name)\n",
    "\n",
    "        for category in ohe.categories_[i]:\n",
    "            new_encoded_column_name = f\"{col}_{category}\"\n",
    "            ohe_mapping[new_encoded_column_name] = (col, category)\n",
    "            new_encoded_columns_per_old_encoded_column[col].add(new_encoded_column_name)\n",
    "\n",
    "    data[encoded_columns] = data_encoded\n",
    "    data[encoded_columns] = data[encoded_columns].astype('category')\n",
    "\n",
    "    # Eliminem les columnes originals\n",
    "    data.drop(columns=columns_to_encode, inplace=True)\n",
    "\n",
    "    if save_to_csv:\n",
    "        # Guardem el dataset\n",
    "        data.to_csv('../assets/data/encoded_cirrhosis.csv', index=False)\n",
    "\n",
    "#encode_variables(data=data, save_to_csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_variables(data: pd.DataFrame, ohe_mapping, original_columns_order):\n",
    "    \"\"\"\n",
    "    Decodifica les variables categòriques que s'hagin codificat anteriorment.\n",
    "    \"\"\"\n",
    "    reconstructed_columns = {}\n",
    "\n",
    "    # Creem les columnes reconstruïdes\n",
    "    for encoded_column in ohe_mapping:\n",
    "        if encoded_column in data.columns:\n",
    "            original_column, category = ohe_mapping[encoded_column]\n",
    "\n",
    "            if original_column not in reconstructed_columns:\n",
    "                reconstructed_columns[original_column] = pd.Series([np.nan] * len(data), index=data.index, dtype='object')\n",
    "\n",
    "            category_rows = data[encoded_column] == 1\n",
    "            reconstructed_columns[original_column].loc[category_rows] = category\n",
    "\n",
    "    # Eliminem les columnes codificades\n",
    "    data.drop(columns=[col for col in ohe_mapping if col in data.columns], inplace=True)\n",
    "\n",
    "    # Inserim les columnes reconstruïdes al DataFrame\n",
    "    for col in reconstructed_columns:\n",
    "        data[col] = reconstructed_columns[col]\n",
    "        data[col] = data[col].astype(original_column_types[col])\n",
    "\n",
    "    # Reordenem les columnes perquè quedin igual que a l'original\n",
    "    data = data.reindex(columns=original_columns_order)\n",
    "\n",
    "#decode_variables(data=data, ohe_mapping=ohe_mapping, original_columns_order=original_columns_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Partició del dataset en train/test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data: pd.DataFrame, test_size: float = 0.15, stratify: bool = True, random_state: int = 42):\n",
    "\t\"\"\"\n",
    "\tParticiona el dataset en conjunts de entrenament i prova.\n",
    "\t\"\"\"\n",
    "\tglobal train, test, X_train, y_train, X_test, y_test\n",
    "\t\n",
    "\tfrom sklearn.model_selection import train_test_split\n",
    "\n",
    "\t# Particionem el dataset en conjunts de entrenament i prova\n",
    "\tif stratify:\n",
    "\t\ttrain, test = train_test_split(data, test_size=test_size, random_state=random_state, stratify=data['Status'])\n",
    "\telse:\n",
    "\t\ttrain, test = train_test_split(data, test_size=test_size, random_state=random_state)\n",
    "\n",
    "\t# 'Status' es la variable target\n",
    "\tX_train = train.drop(columns=['Status'])\n",
    "\ty_train = train['Status']\n",
    "\tX_test = test.drop(columns=['Status'])\n",
    "\ty_test = test['Status']\n",
    "\n",
    "\tprint(f\"Train shape: {train.shape}\")\n",
    "\tprint(f\"Test shape: {test.shape}\")\n",
    "\n",
    "#split_dataset(data=data, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Escalar variables numèriques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_variables(data: pd.DataFrame, scaler: str = 'standard', save_to_csv: bool = True):\n",
    "\tassert scaler in ['standard', 'minmax'], \"El paràmetre 'scaler' ha de ser 'standard' o 'minmax'.\"\n",
    "\t\"\"\"\n",
    "\tEscala les variables numèriques.\n",
    "\t\"\"\"\n",
    "\tfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\tnumerical_columns = data.select_dtypes(include=['Int64', 'float64']).columns\n",
    "\n",
    "\tsc = StandardScaler() if scaler == 'standard' else MinMaxScaler()\n",
    "\n",
    "\tdata[numerical_columns] = sc.fit_transform(data[numerical_columns])\n",
    "\n",
    "\tif save_to_csv:\n",
    "\t\t# Guardem el dataset\n",
    "\t\tdata.to_csv('../assets/data/scaled_cirrhosis.csv', index=False)\n",
    "\n",
    "# scale_variables(data=data, scaler='standard', save_to_csv=True)\n",
    "# scale_variables(data=data, scaler='minmax', save_to_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imputar els valors faltants (Missings)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimeix les variables que tenen valors NaN, el seu percentatge i el seu tipus de dades\n",
    "for col_train in train.columns:\n",
    "\tif train[col_train].isna().any():\n",
    "\t\tprint(f\"{col_train}: {train[col_train].isna().sum()} NaNs ({train[col_train].isna().sum() / len(train) * 100:.2f}%) ({train[col_train].dtype})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_imputer(\n",
    "                X_train: pd.DataFrame, \\\n",
    "                folds_cross_val: int = 5, \\\n",
    "                random_state: int = 42, \\\n",
    "                print_scores: bool = True, \\\n",
    "                name_num_metric: str = 'r2', \\\n",
    "                name_cat_metric: str = 'accuracy'):\n",
    "    \"\"\"\n",
    "    Prova diferents imputadors, imprimeix els seus resultats i retorna el millor.\n",
    "    En variables categòriques binaries, les següents mètriques es converteixen en mètriques binàries (ja que són més adequades per a aquest tipus de variables):\n",
    "        - f1-micro --> f1-binary\n",
    "        - f1-macro --> f1-binary\n",
    "        - f1-weighted --> f1-binary\n",
    "        - precision-micro --> precision-binary\n",
    "        - precision-macro --> precision-binary\n",
    "        - precision-weighted --> precision-binary\n",
    "        - recall-micro --> recall-binary\n",
    "        - recall-macro --> recall-binary\n",
    "        - recall-weighted --> recall-binary\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.metrics import r2_score, accuracy_score, f1_score, precision_score, recall_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "    from sklearn.impute import KNNImputer, SimpleImputer\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.experimental import enable_iterative_imputer\n",
    "    from sklearn.impute import IterativeImputer\n",
    "\n",
    "    assert folds_cross_val > 1, \"El paràmetre 'n_splits' ha de ser major que 1.\"\n",
    "\n",
    "    categorical_binary_columns = ['Ascites', 'Hepatomegaly', 'Spiders']\n",
    "    \n",
    "    # Definir mètriques\n",
    "    f1_micro = lambda y_true, y_pred: f1_score(y_true, y_pred, average='micro')\n",
    "    f1_macro = lambda y_true, y_pred: f1_score(y_true, y_pred, average='macro')\n",
    "    f1_weighted = lambda y_true, y_pred: f1_score(y_true, y_pred, average='weighted')\n",
    "    precision_micro = lambda y_true, y_pred: precision_score(y_true, y_pred, average='micro')\n",
    "    precision_macro = lambda y_true, y_pred: precision_score(y_true, y_pred, average='macro')\n",
    "    precision_weighted = lambda y_true, y_pred: precision_score(y_true, y_pred, average='weighted')\n",
    "    recall_micro = lambda y_true, y_pred: recall_score(y_true, y_pred, average='micro')\n",
    "    recall_macro = lambda y_true, y_pred: recall_score(y_true, y_pred, average='macro')\n",
    "    recall_weighted = lambda y_true, y_pred: recall_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "    fi_binary = lambda y_true, y_pred: f1_score(y_true, y_pred, average='binary')\n",
    "    precision_binary = lambda y_true, y_pred: precision_score(y_true, y_pred, average='binary')\n",
    "    recall_binary = lambda y_true, y_pred: recall_score(y_true, y_pred, average='binary')\n",
    "\n",
    "    # Mapeig de les mètriques\n",
    "    metric_functions = {\n",
    "        'numerical': {\n",
    "            'r2': r2_score, \\\n",
    "            'mse': mean_squared_error, \\\n",
    "            'mae': mean_absolute_error, \\\n",
    "            'mape': mean_absolute_percentage_error\n",
    "        },\n",
    "        'categorical': {\n",
    "            'accuracy': accuracy_score, \\\n",
    "            'f1-micro': f1_micro, \\\n",
    "            'f1-macro': f1_macro, \\\n",
    "            'f1-weighted': f1_weighted, \\\n",
    "            'precision-micro': precision_micro, \\\n",
    "            'precision-macro': precision_macro, \\\n",
    "            'precision-weighted': precision_weighted, \\\n",
    "            'recall-micro': recall_micro, \\\n",
    "            'recall-macro': recall_macro, \\\n",
    "            'recall-weighted': recall_weighted\n",
    "        }                \n",
    "    }\n",
    "\n",
    "    binary_categorical_functions_mapping= {\n",
    "        'f1-micro': fi_binary, \\\n",
    "        'f1-macro': fi_binary, \\\n",
    "        'f1-weighted': fi_binary, \\\n",
    "        'precision-micro': precision_binary, \\\n",
    "        'precision-macro': precision_binary, \\\n",
    "        'precision-weighted': precision_binary, \\\n",
    "        'recall-micro': recall_binary, \\\n",
    "        'recall-macro': recall_binary, \\\n",
    "        'recall-weighted': recall_binary\n",
    "    }\n",
    "\n",
    "    assert name_num_metric in metric_functions['numerical'].keys(), f\"El paràmetre 'name_num_metric' ha de ser una de les següents mètriques: {metric_functions['numerical'].keys()}\"\n",
    "    assert name_cat_metric in metric_functions['categorical'].keys(), f\"El paràmetre 'name_cat_metric' ha de ser una de les següents mètriques: {metric_functions['categorical'].keys()}\"\n",
    "\n",
    "    num_metric = metric_functions['numerical'][name_num_metric]\n",
    "    cat_metric = metric_functions['categorical'][name_cat_metric]\n",
    "\n",
    "    # Definir les particions per a la validació creuada\n",
    "    kf = KFold(n_splits=folds_cross_val, random_state=random_state, shuffle=True)\n",
    "    \n",
    "    # Inicialización de imputadores\n",
    "    imputers = {\n",
    "        'numerical': \n",
    "            {\n",
    "            'KNNImputer-1': KNNImputer(n_neighbors=1),\n",
    "            'KNNImputer-2': KNNImputer(n_neighbors=2),\n",
    "            'KNNImputer-3': KNNImputer(n_neighbors=3),\n",
    "            'KNNImputer-5': KNNImputer(n_neighbors=5),\n",
    "            'KNNImputer-10': KNNImputer(n_neighbors=10),\n",
    "            'KNNImputer-15': KNNImputer(n_neighbors=11),\n",
    "            'KNNImputer-20': KNNImputer(n_neighbors=12),\n",
    "            'KNNImputer-25': KNNImputer(n_neighbors=13),\n",
    "            'KNNImputer-50': KNNImputer(n_neighbors=14),\n",
    "            'SimpleImputer-mean': SimpleImputer(strategy='mean'),\n",
    "            'IterativeImputer-lr': IterativeImputer(estimator=LinearRegression(fit_intercept=True), random_state=random_state),\n",
    "        },\n",
    "\n",
    "        'categorical': \n",
    "            {\n",
    "            'KNeighborsClassifier-1': KNeighborsClassifier(n_neighbors=1),\n",
    "            'KNeighborsClassifier-2': KNeighborsClassifier(n_neighbors=2),\n",
    "            'KNeighborsClassifier-3': KNeighborsClassifier(n_neighbors=3),\n",
    "            'KNeighborsClassifier-5': KNeighborsClassifier(n_neighbors=5),\n",
    "            'KNeighborsClassifier-10': KNeighborsClassifier(n_neighbors=10),\n",
    "            'KNeighborsClassifier-15': KNeighborsClassifier(n_neighbors=15),\n",
    "            'KNeighborsClassifier-20': KNeighborsClassifier(n_neighbors=20),\n",
    "            'KNeighborsClassifier-25': KNeighborsClassifier(n_neighbors=25),\n",
    "            'KNeighborsClassifier-50': KNeighborsClassifier(n_neighbors=50),\n",
    "            'DecisionTreeClassifier-gini': DecisionTreeClassifier(criterion='gini', random_state=random_state),\n",
    "            'DecisionTreeClassifier-entropy': DecisionTreeClassifier(criterion='entropy', random_state=random_state), \n",
    "            'RandomForestClassifier-gini': RandomForestClassifier(criterion='gini', random_state=random_state),\n",
    "            'RandomForestClassifier-entropy': RandomForestClassifier(criterion='entropy', random_state=random_state),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Inicialització dels resultats\n",
    "    scores = {'numerical': {name_num_imputer: [] for name_num_imputer in imputers['numerical'].keys()},\n",
    "              'categorical': {name_cat_imputer: [] for name_cat_imputer in imputers['categorical'].keys()}}\n",
    "    \n",
    "    # Treiem les files amb NaNs\n",
    "    X_train_complete = X_train.dropna()\n",
    "\n",
    "    numerical_columns = X_train_complete.select_dtypes(include=['Int64', 'float64']).columns\n",
    "    categorical_columns = X_train_complete.select_dtypes(include=['category']).columns\n",
    "\n",
    "    for train_index, test_index in kf.split(X_train_complete):\n",
    "        X_train_fold, X_test_fold = X_train_complete.iloc[train_index], X_train_complete.iloc[test_index]\n",
    "\n",
    "        # Imputació i evaluació per a numèriques\n",
    "        for name, imputer in imputers['numerical'].items():\n",
    "            # Introdir artificalment NaNs\n",
    "            X_test_fold_num_with_nan = X_test_fold.copy()\n",
    "            for index in X_test_fold.index:\n",
    "                # Seleccionar aleatoriament una columna numèrica i una categòrica\n",
    "                np.random.seed(random_state)\n",
    "                num_col = np.random.choice(numerical_columns)\n",
    "                X_test_fold_num_with_nan.loc[index, num_col] = np.nan # Només NaNs en les columnes numèriques\n",
    "\n",
    "            imputer.fit(X_train_fold[numerical_columns])\n",
    "            X_test_num_imputed = imputer.transform(X_test_fold_num_with_nan[numerical_columns])\n",
    "            X_test_num_imputed = pd.DataFrame(X_test_num_imputed, columns=numerical_columns, index=X_test_fold_num_with_nan.index)\n",
    "\n",
    "            num_cols_scores = []\n",
    "            for col in numerical_columns:\n",
    "                if X_test_fold_num_with_nan[col].isna().any():\n",
    "                    # Només evaluar les columnes amb NaNs\n",
    "                    true_values = X_test_fold[col]\n",
    "                    imputed_values = X_test_num_imputed[col]\n",
    "                    score = num_metric(true_values, imputed_values)\n",
    "                    num_cols_scores.append(score)\n",
    "                \n",
    "            scores['numerical'][name].append(np.mean(num_cols_scores))\n",
    "                \n",
    "        # Imputació i evaluació per a categòriques\n",
    "        for name, imputer in imputers['categorical'].items():\n",
    "            # Introdir artificalment NaNs\n",
    "            X_test_fold_cat_with_nan = X_test_fold.copy()\n",
    "            for index in X_test_fold.index:\n",
    "                # Seleccionar aleatoriament una columna numèrica i una categòrica\n",
    "                np.random.seed(random_state)\n",
    "                cat_col = np.random.choice(categorical_columns)\n",
    "                X_test_fold_cat_with_nan.loc[index, cat_col] = np.nan # Només NaNs en les columnes categòriques\n",
    "\n",
    "            cat_cols_scores = []\n",
    "            for col in categorical_columns:\n",
    "                if col in categorical_binary_columns and name_cat_metric in binary_categorical_functions_mapping.keys():\n",
    "                    cat_metric = binary_categorical_functions_mapping[name_cat_metric]\n",
    "                else:\n",
    "                    cat_metric = metric_functions['categorical'][name_cat_metric]\n",
    "                imputer.fit(X_train_fold[numerical_columns], X_train_fold[col])\n",
    "                X_test_cat_col_imputed = imputer.predict(X_test_fold_cat_with_nan[numerical_columns])\n",
    "                X_test_cat_col_imputed = pd.Series(X_test_cat_col_imputed, index=X_test_fold_cat_with_nan.index)\n",
    "\n",
    "                true_values = X_test_fold[col]\n",
    "                predicted_values = X_test_cat_col_imputed\n",
    "                score = cat_metric(true_values, predicted_values)\n",
    "                cat_cols_scores.append(score)\n",
    "\n",
    "            scores['categorical'][name].append(np.mean(cat_cols_scores))\n",
    "\n",
    "    # Calcular la mitjana dels resultats\n",
    "    average_scores = {\n",
    "        'numerical': {name: np.mean(scores_val) for name, scores_val in scores['numerical'].items()},\n",
    "        'categorical': {name: np.mean(scores_val) for name, scores_val in scores['categorical'].items()}\n",
    "    }\n",
    "    \n",
    "    # Guardar el millor imputador numèric\n",
    "    best_num_imputer_name = 'None'\n",
    "    best_num_imputer_score = float('-inf') if name_num_metric not in {'mse', 'mae', 'mape'} else float('inf')\n",
    "    num_condition = lambda x, y: x > y if name_num_metric not in {'mse', 'mae', 'mape'} else x < y # Si la mètrica és mse, mae o mape el millor imputador és el que tingui el valor més petit\n",
    "    for name_num_imputer, score_num_imputer in average_scores['numerical'].items():\n",
    "        if num_condition(score_num_imputer, best_num_imputer_score):\n",
    "            best_num_imputer_score = score_num_imputer\n",
    "            best_num_imputer_name = name_num_imputer\n",
    "\n",
    "    # Guardar el millor imputador categòric\n",
    "    best_cat_imputer_name = 'None'\n",
    "    best_cat_imputer_score = float('-inf')\n",
    "    for name_cat_imputer, score_cat_imputer in average_scores['categorical'].items():\n",
    "        if score_cat_imputer > best_cat_imputer_score:\n",
    "            best_cat_imputer_score = score_cat_imputer\n",
    "            best_cat_imputer_name = name_cat_imputer\n",
    "\n",
    "    # Mostrar resultats\n",
    "    if print_scores:\n",
    "        for name_num_imputer, score_num_imputer in average_scores['numerical'].items():\n",
    "            print(f\"NUMERICAL IMPUTER [{name_num_imputer}] --> {score_num_imputer} (\\'{name_num_metric}\\' score)\")\n",
    "\n",
    "        for name_cat_imputer, score_cat_imputer in average_scores['categorical'].items():\n",
    "            print(f\"CATEGORICAL IMPUTER [{name_cat_imputer}] --> {score_cat_imputer} (\\'{name_cat_metric}\\' score)\")\n",
    "        \n",
    "        print(f\"\\nBEST NUMERICAL IMPUTER: {best_num_imputer_name} --> {best_num_imputer_score} (\\'{name_num_metric}\\' score)\")\n",
    "        print(f\"BEST CATEGORICAL IMPUTER: {best_cat_imputer_name} --> {best_cat_imputer_score} (\\'{name_cat_metric}\\' score)\")\n",
    "\n",
    "        if name_num_metric not in {'mse', 'mae', 'mape'}:\n",
    "            print(f\"BEST COMBINED IMPUTER: {best_num_imputer_name} (num) + {best_cat_imputer_name} (cat) --> {np.mean([best_num_imputer_score, best_cat_imputer_score])} (combined \\'{name_num_metric}\\' & \\'{name_cat_metric}\\' scores)\")\n",
    "    \n",
    "    # Retornar els millors imputadors i les seves puntuacions\n",
    "    return {'numerical': {'name': best_num_imputer_name, 'imputer': imputers['numerical'][best_num_imputer_name], 'score': best_num_imputer_score},\n",
    "        'categorical': {'name': best_cat_imputer_name, 'imputer': imputers['categorical'][best_cat_imputer_name], 'score': best_cat_imputer_score},\n",
    "        'combined': {'name': f\"{best_num_imputer_name} (num) + {best_cat_imputer_name} (cat)\", 'score': np.mean([best_num_imputer_score, best_cat_imputer_score])}}\n",
    "\n",
    "#best_imputers = find_best_imputer(X_train=train, n_splits=5, random_state=42, print_scores=True, name_num_metric='r2', name_cat_metric='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_data(data_to_impute: pd.DataFrame, \\\n",
    "\t\t\t\tnumerical_imputer = 'best', \\\n",
    "\t\t\t\tcategorical_imputer = 'best', \\\n",
    "\t\t\t\tsave_to_csv: bool = True, \\\n",
    "\t\t\t\trandom_state: int = 42, \\\n",
    "\t\t\t\tfolds_cross_val: int = 5, \\\n",
    "\t\t\t\tnum_metric = 'r2', \\\n",
    "\t\t\t\tcat_metric = 'accuracy'):\n",
    "\t\"\"\"\n",
    "\tImputa els valors NaN del dataset.\n",
    "\t\"\"\"\n",
    "\tfrom sklearn.impute import KNNImputer, SimpleImputer\n",
    "\tfrom sklearn.neighbors import KNeighborsClassifier\n",
    "\tfrom sklearn.tree import DecisionTreeClassifier\n",
    "\tfrom sklearn.ensemble import RandomForestClassifier\n",
    "\tfrom sklearn.linear_model import LinearRegression\n",
    "\tfrom sklearn.experimental import enable_iterative_imputer\n",
    "\tfrom sklearn.impute import IterativeImputer\n",
    "\t\n",
    "\timputers = {\n",
    "        'numerical': \n",
    "            {\n",
    "            'KNNImputer-1': KNNImputer(n_neighbors=1),\n",
    "            'KNNImputer-2': KNNImputer(n_neighbors=2),\n",
    "            'KNNImputer-3': KNNImputer(n_neighbors=3),\n",
    "            'KNNImputer-5': KNNImputer(n_neighbors=5),\n",
    "            'KNNImputer-10': KNNImputer(n_neighbors=10),\n",
    "            'KNNImputer-15': KNNImputer(n_neighbors=11),\n",
    "            'KNNImputer-20': KNNImputer(n_neighbors=12),\n",
    "            'KNNImputer-25': KNNImputer(n_neighbors=13),\n",
    "            'KNNImputer-50': KNNImputer(n_neighbors=14),\n",
    "            'SimpleImputer-mean': SimpleImputer(strategy='mean'),\n",
    "            'IterativeImputer-lr': IterativeImputer(estimator=LinearRegression(fit_intercept=True), random_state=random_state),\n",
    "        },\n",
    "\n",
    "        'categorical': \n",
    "            {\n",
    "            'KNeighborsClassifier-1': KNeighborsClassifier(n_neighbors=1),\n",
    "            'KNeighborsClassifier-2': KNeighborsClassifier(n_neighbors=2),\n",
    "            'KNeighborsClassifier-3': KNeighborsClassifier(n_neighbors=3),\n",
    "            'KNeighborsClassifier-5': KNeighborsClassifier(n_neighbors=5),\n",
    "            'KNeighborsClassifier-10': KNeighborsClassifier(n_neighbors=10),\n",
    "            'KNeighborsClassifier-15': KNeighborsClassifier(n_neighbors=15),\n",
    "            'KNeighborsClassifier-20': KNeighborsClassifier(n_neighbors=20),\n",
    "            'KNeighborsClassifier-25': KNeighborsClassifier(n_neighbors=25),\n",
    "            'KNeighborsClassifier-50': KNeighborsClassifier(n_neighbors=50),\n",
    "            'DecisionTreeClassifier-gini': DecisionTreeClassifier(criterion='gini', random_state=random_state),\n",
    "            'DecisionTreeClassifier-entropy': DecisionTreeClassifier(criterion='entropy', random_state=random_state), \n",
    "            'RandomForestClassifier-gini': RandomForestClassifier(criterion='gini', random_state=random_state),\n",
    "            'RandomForestClassifier-entropy': RandomForestClassifier(criterion='entropy', random_state=random_state),\n",
    "        }\n",
    "    }\n",
    "\n",
    "\tpossible_numerical_imputers = imputers['numerical'].keys()\n",
    "\tassert numerical_imputer in possible_numerical_imputers or numerical_imputer == 'best', f\"numerical_imputer ha de ser un dels següents valors: {possible_numerical_imputers} (o 'best' per trobar el millor)\"\n",
    "\tpossible_categorical_imputers = imputers['categorical'].keys()\n",
    "\tassert categorical_imputer in possible_categorical_imputers or categorical_imputer == 'best', f\"categorical_imputer ha de ser un dels següents valors: {possible_categorical_imputers} (o 'best' per trobar el millor)\"\n",
    "\n",
    "\tif numerical_imputer == 'best' or categorical_imputer == 'best':\n",
    "\t\tbest_imputers_dict = find_best_imputer(X_train=X_train, \\\n",
    "\t\t\t\t\t\t\t\t\t\trandom_state=random_state, \\\n",
    "\t\t\t\t\t\t\t\t\t\tprint_scores=False, \\\n",
    "\t\t\t\t\t\t\t\t\t\tname_num_metric=num_metric, \\\n",
    "\t\t\t\t\t\t\t\t\t\tname_cat_metric=cat_metric, \\\n",
    "\t\t\t\t\t\t\t\t\t\tfolds_cross_val=folds_cross_val)\n",
    "\n",
    "\t\tif numerical_imputer == 'best' and categorical_imputer == 'best':\n",
    "\t\t\tnumerical_imputer = best_imputers_dict['numerical']['imputer']\n",
    "\t\t\tcategorical_imputer = best_imputers_dict['categorical']['imputer']\n",
    "\t\t\tprint(f\"IMPUTER COMBINAT ESCOLLIT: {best_imputers_dict['combined']['name']} --> {best_imputers_dict['combined']['score']} (combinació de \\'{num_metric}\\' i \\'{cat_metric}\\' en X_train)\")\n",
    "\t\telse:\n",
    "\t\t\tif numerical_imputer == 'best':\n",
    "\t\t\t\tnumerical_imputer = best_imputers_dict['numerical']['imputer']\n",
    "\t\t\t\tprint(f\"IMPUTER NUMÈRIC ESCOLLIT: {best_imputers_dict['numerical']['name']} --> {best_imputers_dict['numerical']['score']} (\\'{num_metric}\\' en X_train)\")\n",
    "\t\t\telse:\n",
    "\t\t\t\tnumerical_imputer = imputers['numerical'][numerical_imputer]\n",
    "\n",
    "\t\t\tif categorical_imputer == 'best':\n",
    "\t\t\t\tcategorical_imputer = best_imputers_dict['categorical']['imputer']\n",
    "\t\t\t\tprint(f\"IMPUTER CATEGÒRIC ESCOLLIT: {best_imputers_dict['categorical']['name']} --> {best_imputers_dict['categorical']['score']} (\\'{cat_metric}\\' en X_train)\")\n",
    "\t\t\telse:\n",
    "\t\t\t\tcategorical_imputer = imputers['categorical'][categorical_imputer]\n",
    "\t\n",
    "\telse:\n",
    "\t\tnumerical_imputer = imputers['numerical'][numerical_imputer]\n",
    "\t\tcategorical_imputer = imputers['categorical'][categorical_imputer]\n",
    "\n",
    "\n",
    "\tX_train_complete = X_train.dropna()\n",
    "\n",
    "\tnumerical_cols = data_to_impute.select_dtypes(include=['Int64', 'float64']).columns\n",
    "\tcategorical_cols = data_to_impute.select_dtypes(include=['category']).columns\n",
    "\n",
    "\n",
    "\t# Imputar numèriques\n",
    "\tnumerical_imputer.fit(X_train_complete[numerical_cols])\n",
    "\timputed_num_cols = numerical_imputer.transform(data_to_impute[numerical_cols])\n",
    "\timputed_num_cols = pd.DataFrame(imputed_num_cols, columns=numerical_cols, index=data_to_impute.index)\n",
    "\tdata_to_impute[numerical_cols] = imputed_num_cols\n",
    "\n",
    "\t# Imputar categòriques\n",
    "\tfor col in categorical_cols:\n",
    "\t\tif data_to_impute[col].isna().any():\n",
    "\t\t\tcategorical_imputer.fit(X_train_complete[numerical_cols], X_train_complete[col])\n",
    "\t\t\timputed_cat_col = categorical_imputer.predict(data_to_impute[numerical_cols])\n",
    "\t\t\timputed_cat_col = pd.Series(imputed_cat_col, index=data_to_impute[col].index)\n",
    "\t\t\tdata_to_impute[col] = imputed_cat_col\n",
    "\n",
    "\t# Comprovar que no queden NaNs\n",
    "\tassert not data_to_impute.isna().any().any(), \"No s'han pogut eliminar tots els NaNs del dataset.\"\n",
    "\n",
    "\t# Guardar el dataset\n",
    "\tif save_to_csv:\n",
    "\t\tdata_to_impute.to_csv('../assets/data/imputed_cirrhosis.csv', index=False)\n",
    "\n",
    "#impute_data(data_to_impute=X_train, imputer='best', save_to_csv=True, random_state=42, encode=True, decode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Balanceig de classes de la variable objectiu (Status)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_target_classes(X_data: pd.DataFrame, y_data: pd.Series, method: str ='oversample', random_state: int = 42):\n",
    "\t\"\"\"\n",
    "\tBalanceja les classes de la variable target utilitzant el mètode especificat. X_data no han de contenir NaNs.\n",
    "\t\"\"\"\n",
    "\tfrom imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "\tfrom imblearn.under_sampling import RandomUnderSampler\n",
    "\tfrom imblearn.combine import SMOTEENN\n",
    "\t\n",
    "\tmethods = {\n",
    "\t\t'oversample': RandomOverSampler(random_state=random_state),\n",
    "\t\t'undersample': RandomUnderSampler(random_state=random_state),\n",
    "\t\t'smote': SMOTE(random_state=random_state),\n",
    "\t\t'smoteenn': SMOTEENN(random_state=random_state)\n",
    "\t}\n",
    "\n",
    "\tassert method in methods.keys(), f\"El paràmetre 'method' ha de ser un dels següents: {methods.keys()}\"\n",
    "\n",
    "\tencode = True if method in {'smote', 'smoteenn'} else False\n",
    "\n",
    "\tif encode:\n",
    "\t\t# Cal fer OneHotEncoding a les variables categòriques perquè SMOTE o SMOTEENN les puguin tractar\n",
    "\t\tencode_variables(data=X_data, save_to_csv=False)\n",
    "\n",
    "\tprint(f\"Mostres per classe abans de balancejar ({method}):\")\n",
    "\tfor class_, count in y_data.value_counts().items():\n",
    "\t\tprint(f\"\\t*{class_}: {count}\")\n",
    "\tprint(f\"\\t*Total de mostres: {len(y_data)}\")\n",
    "\n",
    "\t# Seleccionar el mètode de balanceig\n",
    "\tbalancer = methods[method]\n",
    "\n",
    "\t# Balancejar les classes\n",
    "\tX_balanced, y_balanced = balancer.fit_resample(X_data, y_data)\n",
    "\n",
    "\t# Convertir a DataFrame i Series\n",
    "\tX_balanced = pd.DataFrame(X_balanced, columns=X_data.columns)\n",
    "\ty_balanced = pd.Series(y_balanced, name=y_data.name)\n",
    "\n",
    "\tif encode:\n",
    "\t\t# Decodificar les variables categòriques\n",
    "\t\tdecode_variables(data=X_balanced, ohe_mapping=ohe_mapping, original_columns_order=original_columns_order)\n",
    "\t\t\n",
    "\t# Guardar el dataset en l'entorn global\n",
    "\tglobal X_train, y_train\n",
    "\tX_train = X_balanced\n",
    "\ty_train = y_balanced\n",
    "\t\n",
    "\tprint(f\"Mostres per classe després de balancejar ({method}):\")\n",
    "\tfor class_, count in y_balanced.value_counts().items():\n",
    "\t\tprint(f\"\\t*{class_}: {count}\")\n",
    "\tprint(f\"\\t*Total de mostres: {len(y_balanced)}\")\n",
    "\n",
    "# balance_target_classes(X_data=X_train, y_data=y_train, method='smoteenn', random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Correlacions entre variables numèriques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_vars_correlations(data: pd.DataFrame):\n",
    "\t\"\"\"\n",
    "\tVisualitza la correlació entre les variables numèriques.\n",
    "\t\"\"\"\n",
    "\n",
    "\tnumerical_columns = data.select_dtypes(include=['Int64', 'float64']).columns\n",
    "\n",
    "\tplt.figure(figsize=(10, 6))\n",
    "\n",
    "\tsns.heatmap(data[numerical_columns].corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "\n",
    "\tplt.title('Correlació entre les variables numèriques')\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()\n",
    "\n",
    "\n",
    "numerical_vars_correlations(data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Correlacions entre variables categòriques i variable objectiu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_target_relationships(data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Visualitza la relació entre les variables categòriques i la variable target.\n",
    "    \"\"\"\n",
    "    categorical_columns = data.select_dtypes(include=['category']).columns\n",
    "\n",
    "    for col in categorical_columns:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        sns.countplot(data=data, x=col, hue='Status')\n",
    "        plt.title(f'Distribució de {col} vs Status (target)')\n",
    "        plt.show()\n",
    "\n",
    "categorical_target_relationships(data=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Anàlisis de Components Principals (ACP)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def principal_component_analysis(data: pd.DataFrame):\n",
    "\t\"\"\"\n",
    "\tAplica PCA para explicar el 80% de la varianza y visualiza los resultados.\n",
    "\n",
    "\t:param X: DataFrame con los datos numéricos.\n",
    "\t:param categorical_columns: Lista de nombres de las columnas categóricas para análisis de centroides.\n",
    "\t\"\"\"\n",
    "\tfrom sklearn.decomposition import PCA\n",
    "\t# Seleccionar variables numéricas\n",
    "\tnumerical_columns = data.select_dtypes(include=['Int64', 'float64']).columns\n",
    "\tcategorical_columns = data.select_dtypes(include=['category']).columns\n",
    "\n",
    "\t# Aplicar PCA\n",
    "\tpca = PCA(n_components=0.80)\n",
    "\tX_pca = pca.fit_transform(data[numerical_columns])\n",
    "\n",
    "\t# Variància acumulada\n",
    "\tplt.figure(figsize=(10, 6))\n",
    "\tplt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "\tplt.xlabel('Número de components')\n",
    "\tplt.ylabel('Variància acumulada')\n",
    "\tplt.title('Anàlisi de de la variància acumulada en funció del número de components')\n",
    "\tplt.grid(True)\n",
    "\tplt.show()\n",
    "\n",
    "\t# Gràfic de barres amb una línia que marca el 80% de la variància\n",
    "\tplt.figure(figsize=(10, 6))\n",
    "\tplt.bar(range(len(pca.explained_variance_ratio_)), pca.explained_variance_ratio_)\n",
    "\tplt.plot(range(len(pca.explained_variance_ratio_)), np.cumsum(pca.explained_variance_ratio_), c='red')\n",
    "\tplt.xlabel('Número de components')\n",
    "\tplt.ylabel('Variància')\n",
    "\tplt.title('Anàlisi de la variància en funció del número de components')\n",
    "\tplt.grid(True)\n",
    "\tplt.show()\n",
    "\n",
    "\t# Projecció de les variables numèriques\n",
    "\tplt.figure(figsize=(10, 6))\n",
    "\tplt.scatter(X_pca[:, 0], X_pca[:, 1])\n",
    "\tinercia_primer_comp = round(pca.explained_variance_ratio_[0] * 100, 2)\n",
    "\tinercia_segon_comp= round(pca.explained_variance_ratio_[1] * 100, 2)\n",
    "\tplt.xlabel(f'1r component principal ({inercia_primer_comp}%)')\n",
    "\tplt.ylabel(f'2n component principal ({inercia_segon_comp}%)')\n",
    "\tplt.title('Projecció de les variables numèriques sobre els dos primers components principals')\n",
    "\tplt.grid(True)\n",
    "\n",
    "\t# Projecció de les variables categòriques\n",
    "\tfor col in categorical_columns:\n",
    "\t\tif col in data.columns:\n",
    "\t\t\tfor category in np.unique(data[col]):\n",
    "\t\t\t\tmask = data[col] == category\n",
    "\t\t\t\tcentroid = np.mean(X_pca[mask], axis=0)\n",
    "\t\t\t\tplt.scatter(centroid[0], centroid[1], marker='x', color='red')\n",
    "\tplt.legend()\n",
    "\tplt.show()\n",
    "\n",
    "principal_component_analysis(data=train.dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1r Model: K-Nearest Neighbors (KNN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_prediction(X_train: pd.DataFrame, y_train: pd.Series, X_test: pd.DataFrame, y_test: pd.Series, \\\n",
    "\t\t\t\t\tk_list: list[int] = [1, 2, 3, 5, 10, 15, 20, 25, 50], \\\n",
    "\t\t\t\t\tscorer_cross_val: str = 'accuracy', \\\n",
    "\t\t\t\t\tfolds_cross_val: int = 5, \\\n",
    "\t\t\t\t\tplot_confusion_matrix: bool = True, \\\n",
    "\t\t\t\t\tprint_scores: bool = True, \\\n",
    "\t\t\t\t\tencode: bool = False):\n",
    "\t\"\"\"\n",
    "\tBusca el millor valor de k per a KNN utilitzant cross validation i mostra els resultats.\n",
    "\t\"\"\"\n",
    "\tfrom sklearn.neighbors import KNeighborsClassifier\n",
    "\tfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay, make_scorer\n",
    "\tfrom sklearn.model_selection import cross_val_score\n",
    "\n",
    "\tassert all([k > 0 for k in k_list]), \"Els valors de k han de ser positius.\"\n",
    "\tassert folds_cross_val > 1, \"El paràmetre 'folds_cross_val' ha de ser major que 1.\"\n",
    "\n",
    "\tscorers_cross_val = {\n",
    "\t\t'accuracy': make_scorer(accuracy_score),\n",
    "\t\t'f1-micro': make_scorer(f1_score, average='micro'),\n",
    "\t\t'f1-macro': make_scorer(f1_score, average='macro'),\n",
    "\t\t'f1-weighted': make_scorer(f1_score, average='weighted'),\n",
    "\t\t'precision-micro': make_scorer(precision_score, average='micro'),\n",
    "\t\t'precision-macro': make_scorer(precision_score, average='macro'),\n",
    "\t\t'precision-weighted': make_scorer(precision_score, average='weighted'),\n",
    "\t\t'recall-micro': make_scorer(recall_score, average='micro'),\n",
    "\t\t'recall-macro': make_scorer(recall_score, average='macro'),\n",
    "\t\t'recall-weighted': make_scorer(recall_score, average='weighted')\n",
    "\t}\n",
    "\n",
    "\tassert scorer_cross_val in scorers_cross_val.keys(), f\"El paràmetre 'scorer_cross_val' ha de ser un dels següents: {scorers_cross_val.keys()}\"\n",
    "\n",
    "\t# Encode\n",
    "\tif encode:\n",
    "\t\tencode_variables(data=X_train, save_to_csv=True)\n",
    "\t\tencode_variables(data=X_test, save_to_csv=False)\n",
    "\t\tX_train_ready = X_train\n",
    "\t\tX_test_ready = X_test\n",
    "\telse:\n",
    "\t\t# Només variables numèriques per a KNN\n",
    "\t\tnumerical_columns = X_train.select_dtypes(include=['Int64', 'float64']).columns\n",
    "\t\tX_train_ready = X_train[numerical_columns]\n",
    "\t\tX_test_ready = X_test[numerical_columns]\n",
    "\t\t\t\n",
    "\t# Cross validation per a cada valor de k\n",
    "\tscores_cross_val = {}\n",
    "\tfor k in k_list:\n",
    "\t\tknn = KNeighborsClassifier(n_neighbors=k)\n",
    "\t\tscores_cross_val[k] = np.mean([cross_val_score(knn, X_train_ready, y_train, cv=folds_cross_val, scoring=scorers_cross_val[scorer_cross_val])])\n",
    "\n",
    "\tbest_k = max(scores_cross_val, key=lambda k: scores_cross_val[k])\n",
    "\tbest_knn = KNeighborsClassifier(n_neighbors=best_k)\n",
    "\n",
    "\tif print_scores:\n",
    "\t\tprint(\"\\nMODEL K-NEAREST NEIGHBORS (KNN)\")\n",
    "\t\tprint(f\"Millors Paràmetres Cross Validation: k={best_k}\")\n",
    "\t\tprint(f\"Millor Score Cross Validation ({scorer_cross_val}) --> {scores_cross_val[best_k]}\")\n",
    "\n",
    "\t# Realitzem la predicció\n",
    "\tbest_knn.fit(X_train_ready, y_train)\n",
    "\ty_pred = best_knn.predict(X_test_ready)\n",
    "\n",
    "\t# Resultats\n",
    "\tscores = {\n",
    "\t\t'accuracy': accuracy_score(y_test, y_pred),\n",
    "\t\t'f1-micro': f1_score(y_test, y_pred, average='micro'),\n",
    "\t\t'f1-macro': f1_score(y_test, y_pred, average='macro'),\n",
    "\t\t'f1-weighted': f1_score(y_test, y_pred, average='weighted'),\n",
    "\t\t'precision-micro': precision_score(y_test, y_pred, average='micro'),\n",
    "\t\t'precision-macro': precision_score(y_test, y_pred, average='macro'),\n",
    "\t\t'precision-weighted': precision_score(y_test, y_pred, average='weighted'),\n",
    "\t\t'recall-micro': recall_score(y_test, y_pred, average='micro'),\n",
    "\t\t'recall-macro': recall_score(y_test, y_pred, average='macro'),\n",
    "\t\t'recall-weighted': recall_score(y_test, y_pred, average='weighted')\n",
    "\t}\n",
    "\n",
    "\t# Mostrar els resultats\n",
    "\tif print_scores:\n",
    "\t\tprint(f\"SCORES BEST KNN (k={best_k})\")\n",
    "\t\tfor metric in scores:\n",
    "\t\t\tprint(f\"\\t*{metric} --> {scores[metric]}\")\n",
    "\t\tprint(f\"\\t*Average Metrics Score --> {np.mean(list(scores.values()))}\")\n",
    "\t\n",
    "\t# Decode\n",
    "\tif encode:\n",
    "\t\tdecode_variables(data=X_train, ohe_mapping=ohe_mapping, original_columns_order=original_columns_order)\n",
    "\t\tdecode_variables(data=X_test, ohe_mapping=ohe_mapping, original_columns_order=original_columns_order)\n",
    "\n",
    "\tif plot_confusion_matrix:\n",
    "\t\t# Matriu de confusió \n",
    "\t\tunique_classes = np.unique(y_test)\n",
    "\t\tcm = confusion_matrix(y_test, y_pred, labels=unique_classes)\n",
    "\t\tConfusionMatrixDisplay(cm, display_labels=unique_classes).plot()\n",
    "\t\tplt.title(f\"Matriu de Confusió - KNN (k={best_k})\")\n",
    "\t\tplt.show()\n",
    "\n",
    "\treturn scores, best_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2n Model: Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt_prediction(X_train: pd.DataFrame, y_train: pd.Series, X_test: pd.DataFrame, y_test: pd.Series,\n",
    "\t\t\t\t  \tcriterion_list: list[str] = ['entropy', 'gini'], \\\n",
    "\t\t\t\t\tmax_depth_list: list[int|None] = [None, 2, 3, 5, 10, 15, 20, 25, 50],\n",
    "\t\t\t\t  \tmin_samples_split_list: list[int] = [2, 3, 5, 10, 15, 20, 25, 50], \\\n",
    "\t\t\t\t\tmin_samples_leaf_list: list[int] = [1, 2, 3, 5, 10, 15, 20, 25, 50],\n",
    "\t\t\t\t  \trandom_state: int = 42, \\\n",
    "\t\t\t\t\tfolds_cross_val: int = 5, \\\n",
    "\t\t\t\t\tscorer_cross_val: str = 'accuracy',\n",
    "\t\t\t\t  \tplot_confusion_matrix: bool = True, \\\n",
    "\t\t\t\t\tprint_scores: bool = True):\n",
    "\t\"\"\"\n",
    "\tBusca el millor model de Decision Tree utilitzant la validació creuada i fa la predicció amb els millors paràmetres.\n",
    "\t\"\"\"\n",
    "\tfrom sklearn.tree import DecisionTreeClassifier\n",
    "\tfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay, make_scorer\n",
    "\tfrom sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\tassert all([max_depth is None or max_depth > 0 for max_depth in max_depth_list]), \"Els valors de max_depth han de ser positius o None.\"\n",
    "\tassert all([min_samples_split > 1 for min_samples_split in min_samples_split_list]), \"Els valors de min_samples_split han de ser majors que 1.\"\n",
    "\tassert all([min_samples_leaf > 0 for min_samples_leaf in min_samples_leaf_list]), \"Els valors de min_samples_leaf han de ser positius.\"\n",
    "\tassert folds_cross_val > 1, \"El paràmetre 'cross_val_folds' ha de ser major que 1.\"\n",
    "\tassert all([criterion in {'entropy', 'gini'} for criterion in criterion_list]), \"Els valors de criterion han de ser 'entropy' o 'gini'.\"\n",
    "\n",
    "\tscorers_cross_val = {\n",
    "\t\t'accuracy': make_scorer(accuracy_score),\n",
    "\t\t'f1-micro': make_scorer(f1_score, average='micro'),\n",
    "\t\t'f1-macro': make_scorer(f1_score, average='macro'),\n",
    "\t\t'f1-weighted': make_scorer(f1_score, average='weighted'),\n",
    "\t\t'precision-micro': make_scorer(precision_score, average='micro'),\n",
    "\t\t'precision-macro': make_scorer(precision_score, average='macro'),\n",
    "\t\t'precision-weighted': make_scorer(precision_score, average='weighted'),\n",
    "\t\t'recall-micro': make_scorer(recall_score, average='micro'),\n",
    "\t\t'recall-macro': make_scorer(recall_score, average='macro'),\n",
    "\t\t'recall-weighted': make_scorer(recall_score, average='weighted')\n",
    "\t}\n",
    "\n",
    "\tassert scorer_cross_val in scorers_cross_val.keys(), f\"El paràmetre 'scorer_cross_val' ha de ser un dels següents: {scorers_cross_val.keys()}\"\n",
    "\n",
    "\t# Configurar la cerca dels millors paràmetres\n",
    "\tparam_grid = {\n",
    "\t\t'criterion': criterion_list,\n",
    "\t\t'max_depth': max_depth_list or [None],\n",
    "\t\t'min_samples_split': min_samples_split_list or [2],\n",
    "\t\t'min_samples_leaf': min_samples_leaf_list or [1]\n",
    "\t}\n",
    "\n",
    "\tdt = DecisionTreeClassifier(random_state=random_state)\t\n",
    "\tgrid_search = GridSearchCV(dt, param_grid, cv=folds_cross_val, scoring=scorers_cross_val[scorer_cross_val])\n",
    "\tgrid_search.fit(X_train, y_train)\n",
    "\n",
    "\tbest_dt = grid_search.best_estimator_\n",
    "\tbest_params = grid_search.best_params_\n",
    "\n",
    "\tif print_scores:\n",
    "\t\tprint(\"\\nMODEL DECISION TREE\")\n",
    "\t\tprint(f\"Millors Paràmetres Cross Validation: {best_params}\")\n",
    "\t\tprint(f\"Millor Score Cross Validation ({scorer_cross_val}) --> {grid_search.best_score_}\")\n",
    "\n",
    "\t# Predicció del millor model\n",
    "\ty_pred = best_dt.predict(X_test)\n",
    "\n",
    "\t# Resultats\n",
    "\tscores = {\n",
    "\t\t'accuracy': accuracy_score(y_test, y_pred),\n",
    "\t\t'f1-micro': f1_score(y_test, y_pred, average='micro'),\n",
    "\t\t'f1-macro': f1_score(y_test, y_pred, average='macro'),\n",
    "\t\t'f1-weighted': f1_score(y_test, y_pred, average='weighted'),\n",
    "\t\t'precision-micro': precision_score(y_test, y_pred, average='micro'),\n",
    "\t\t'precision-macro': precision_score(y_test, y_pred, average='macro'),\n",
    "\t\t'precision-weighted': precision_score(y_test, y_pred, average='weighted'),\n",
    "\t\t'recall-micro': recall_score(y_test, y_pred, average='micro'),\n",
    "\t\t'recall-macro': recall_score(y_test, y_pred, average='macro'),\n",
    "\t\t'recall-weighted': recall_score(y_test, y_pred, average='weighted')\n",
    "\t}\n",
    "\n",
    "\t# Mostrar resultats\n",
    "\tif print_scores:\n",
    "\t\tprint(f\"SCORES BEST DECISION TREE ({best_params})\")\n",
    "\t\tfor metric, score in scores.items():\n",
    "\t\t\tprint(f\"\\t*{metric}: {score}\")\n",
    "\n",
    "\t# Mostrar la matriu de confusió\n",
    "\tif plot_confusion_matrix:\n",
    "\t\tunique_classes = np.unique(y_test)\n",
    "\t\tcm = confusion_matrix(y_test, y_pred, labels=unique_classes)\n",
    "\t\tConfusionMatrixDisplay(cm, display_labels=unique_classes).plot()\n",
    "\t\tplt.title(f\"Matriu de Confusió - Decision Tree ({best_params})\")\n",
    "\t\tplt.show()\n",
    "\n",
    "\treturn scores, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3r Model: Support Vector Machine (SVM)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_prediction(X_train: pd.DataFrame, y_train: pd.Series, X_test: pd.DataFrame, y_test: pd.Series, \\\n",
    "\t\t\t\t\tkernel_list: list[str] = ['linear', 'poly', 'rbf', 'sigmoid'], \\\n",
    "\t\t\t\t\tC_list: list[float] = [0.1, 0.5, 1, 2, 5, 10, 20, 50, 100], \\\n",
    "\t\t\t\t\tgamma_list: list[str] = ['scale', 'auto'], \\\n",
    "\t\t\t\t\tscorer_cross_val: str = 'accuracy', \\\n",
    "\t\t\t\t\tfolds_cross_val: int = 5, \\\n",
    "\t\t\t\t\tprint_scores: bool = True, \\\n",
    "\t\t\t\t\tplot_confusion_matrix: bool = True, \\\n",
    "\t\t\t\t\trandom_state: int = 42):\n",
    "\t\"\"\"\n",
    "\tBusca el millor model SVM utilitzant la validació creuada i fa la predicció amb els millors paràmetres.\n",
    "\t\"\"\"\n",
    "\tfrom sklearn.svm import SVC\n",
    "\tfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay, make_scorer\n",
    "\tfrom sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\tassert all([C > 0 for C in C_list]), \"Els valors de C han de ser positius.\"\n",
    "\tassert folds_cross_val > 1, \"El paràmetre 'cross_val_folds' ha de ser major que 1.\"\n",
    "\tassert all([kernel in {'linear', 'poly', 'rbf', 'sigmoid'} for kernel in kernel_list]), \"Els valors de kernel han de ser 'linear', 'poly', 'rbf' o 'sigmoid'.\"\n",
    "\tassert all([gamma in {'scale', 'auto'} for gamma in gamma_list]), \"Els valors de gamma han de ser 'scale' o 'auto'.\"\n",
    "\n",
    "\tscorers_cross_val = {\n",
    "\t\t'accuracy': make_scorer(accuracy_score),\n",
    "\t\t'f1-micro': make_scorer(f1_score, average='micro'),\n",
    "\t\t'f1-macro': make_scorer(f1_score, average='macro'),\n",
    "\t\t'f1-weighted': make_scorer(f1_score, average='weighted'),\n",
    "\t\t'precision-micro': make_scorer(precision_score, average='micro'),\n",
    "\t\t'precision-macro': make_scorer(precision_score, average='macro'),\n",
    "\t\t'precision-weighted': make_scorer(precision_score, average='weighted'),\n",
    "\t\t'recall-micro': make_scorer(recall_score, average='micro'),\n",
    "\t\t'recall-macro': make_scorer(recall_score, average='macro'),\n",
    "\t\t'recall-weighted': make_scorer(recall_score, average='weighted')\n",
    "\t}\n",
    "\n",
    "\tassert scorer_cross_val in scorers_cross_val.keys(), f\"El paràmetre 'scorer_cross_val' ha de ser un dels següents: {scorers_cross_val.keys()}\"\n",
    "\n",
    "\t# Configurar la cerca dels millors paràmetres\n",
    "\tparam_grid = {\n",
    "\t\t'kernel': kernel_list,\n",
    "\t\t'C': C_list,\n",
    "\t\t'gamma': gamma_list\n",
    "\t}\n",
    "\n",
    "\tsvm = SVC(random_state=random_state)\n",
    "\tgrid_search = GridSearchCV(svm, param_grid, cv=folds_cross_val, scoring=scorers_cross_val[scorer_cross_val])\n",
    "\tgrid_search.fit(X_train, y_train)\n",
    "\n",
    "\tbest_svm = grid_search.best_estimator_\n",
    "\tbest_params = grid_search.best_params_\n",
    "\n",
    "\tif print_scores:\n",
    "\t\tprint(\"\\nMODEL SUPPORT VECTOR MACHINE (SVM)\")\n",
    "\t\tprint(f\"Millors Paràmetres Cross Validation: {best_params}\")\n",
    "\t\tprint(f\"Millor Score Cross Validation ({scorer_cross_val}) --> {grid_search.best_score_}\")\n",
    "\n",
    "\t# Predicció del millor model\n",
    "\ty_pred = best_svm.predict(X_test)\n",
    "\n",
    "\t# Resultats\n",
    "\tscores = {\n",
    "\t\t'accuracy': accuracy_score(y_test, y_pred),\n",
    "\t\t'f1-micro': f1_score(y_test, y_pred, average='micro'),\n",
    "\t\t'f1-macro': f1_score(y_test, y_pred, average='macro'),\n",
    "\t\t'f1-weighted': f1_score(y_test, y_pred, average='weighted'),\n",
    "\t\t'precision-micro': precision_score(y_test, y_pred, average='micro'),\n",
    "\t\t'precision-macro': precision_score(y_test, y_pred, average='macro'),\n",
    "\t\t'precision-weighted': precision_score(y_test, y_pred, average='weighted'),\n",
    "\t\t'recall-micro': recall_score(y_test, y_pred, average='micro'),\n",
    "\t\t'recall-macro': recall_score(y_test, y_pred, average='macro'),\n",
    "\t\t'recall-weighted': recall_score(y_test, y_pred, average='weighted')\n",
    "\t}\n",
    "\n",
    "\t# Mostrar resultats\n",
    "\tif print_scores:\n",
    "\t\tprint(f\"SCORES BEST SVM ({best_params})\")\n",
    "\t\tfor metric, score in scores.items():\n",
    "\t\t\tprint(f\"\\t*{metric}: {score}\")\n",
    "\n",
    "\t# Mostrar la matriu de confusió\n",
    "\tif plot_confusion_matrix:\n",
    "\t\tunique_classes = np.unique(y_test)\n",
    "\t\tcm = confusion_matrix(y_test, y_pred, labels=unique_classes)\n",
    "\t\tConfusionMatrixDisplay(cm, display_labels=unique_classes).plot()\n",
    "\t\tplt.title(f\"Matriu de Confusió - SVM ({best_params})\")\n",
    "\t\tplt.show()\n",
    "\n",
    "\treturn scores, best_params\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random state: 42\n",
      "Train shape: (355, 19)\n",
      "Test shape: (63, 19)\n",
      "IMPUTER COMBINAT ESCOLLIT: KNNImputer-50 (num) + RandomForestClassifier-gini (cat) --> 0.3244969179652108 (combinació de 'r2' i 'f1-weighted' en X_train)\n",
      "IMPUTER COMBINAT ESCOLLIT: KNNImputer-50 (num) + RandomForestClassifier-gini (cat) --> 0.3244969179652108 (combinació de 'r2' i 'f1-weighted' en X_train)\n",
      "Mostres per classe abans de balancejar (smote):\n",
      "\t*Alive: 197\n",
      "\t*Dead: 137\n",
      "\t*LiverTransplant: 21\n",
      "\t*Total de mostres: 355\n",
      "Mostres per classe després de balancejar (smote):\n",
      "\t*Alive: 197\n",
      "\t*LiverTransplant: 197\n",
      "\t*Dead: 197\n",
      "\t*Total de mostres: 591\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "DESPRES BALANCE.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Cai Selvas Sala\\GIA_UPC\\2nC\\1rQ\\IAA\\Practica\\src\\main.ipynb Cell 53\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y103sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m X_train\u001b[39m.\u001b[39misna()\u001b[39m.\u001b[39many()\u001b[39m.\u001b[39many(), \u001b[39m\"\u001b[39m\u001b[39mDESPRES DECODE 1.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y103sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m balance_target_classes(X_data\u001b[39m=\u001b[39mX_train, y_data\u001b[39m=\u001b[39my_train, method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msmote\u001b[39m\u001b[39m'\u001b[39m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y103sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m X_train\u001b[39m.\u001b[39misna()\u001b[39m.\u001b[39many()\u001b[39m.\u001b[39many(), \u001b[39m\"\u001b[39m\u001b[39mDESPRES BALANCE.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y103sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m _ \u001b[39m=\u001b[39m knn_prediction(X_train\u001b[39m=\u001b[39mX_train, y_train\u001b[39m=\u001b[39my_train, X_test\u001b[39m=\u001b[39mX_test, y_test\u001b[39m=\u001b[39my_test, \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y103sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \t\t\t   \tk_list\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39m15\u001b[39m, \u001b[39m20\u001b[39m, \u001b[39m25\u001b[39m, \u001b[39m50\u001b[39m], \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y103sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \t\t\t\tscorer_cross_val\u001b[39m=\u001b[39mcat_metric, \\\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y103sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \t\t\t\tplot_confusion_matrix\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y103sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \t\t\t\tprint_scores\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y103sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m#_ = dt_prediction(X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, \\criterion_list=['entropy', 'gini'], max_depth_list=[None, 2, 3, 5, 10, 15, 20, 25, 50], min_samples_split_list=[2, 3, 5, 10, 15, 20, 25, 50], min_samples_leaf_list=[1, 2, 3, 5, 10, 15, 20, 25, 50], random_state=random_state, folds_cross_val=folds, scorer_cross_val=cat_metric, plot_confusion_matrix=False, print_scores=True)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cai%20Selvas%20Sala/GIA_UPC/2nC/1rQ/IAA/Practica/src/main.ipynb#Y103sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m#_ = svm_prediction(X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, kernel_list=['linear', 'poly', 'rbf', 'sigmoid'], C_list=[0.1, 0.5, 1, 2, 5, 10, 20, 50, 100], gamma_list=['scale', 'auto'], scorer_cross_val='cat_metric', folds_cross_val=folds, print_scores=True, plot_confusion_matrix=False, random_state=random_state)\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: DESPRES BALANCE."
     ]
    }
   ],
   "source": [
    "random_state = 42\n",
    "#random_state = np.random.randint(0, 1000)\n",
    "print(f\"Random state: {random_state}\")\n",
    "num_metric = 'r2'\n",
    "cat_metric = 'f1-weighted'\n",
    "folds = 5\n",
    "load_dataset(save_to_csv=False)\n",
    "initial_preprocessing(data=data, save_to_csv=False)\n",
    "#delete_outliers(data=data, factor=3, plots=False, save_to_csv=False)\n",
    "#delete_last_rows(data=data, save_to_csv=False)\n",
    "split_dataset(data=data, test_size=0.15, random_state=random_state)\n",
    "scale_variables(data=X_train, scaler='minmax', save_to_csv=False)\n",
    "scale_variables(data=X_test, scaler='minmax', save_to_csv=False)\n",
    "impute_data(data_to_impute=X_test, \\\n",
    "\t\t\tnumerical_imputer='best', \\\n",
    "\t\t\tcategorical_imputer='best', \\\n",
    "\t\t\tsave_to_csv=False, \\\n",
    "\t\t\trandom_state=random_state, \\\n",
    "\t\t\tfolds_cross_val=folds, \\\n",
    "\t\t\tnum_metric=num_metric, \\\n",
    "\t\t\tcat_metric=cat_metric)\n",
    "impute_data(data_to_impute=X_train, \\\n",
    "\t\t\tnumerical_imputer='best', \\\n",
    "\t\t\tcategorical_imputer='best', \\\n",
    "\t\t\tsave_to_csv=False, \\\n",
    "\t\t\trandom_state=random_state, \\\n",
    "\t\t\tfolds_cross_val=folds, \\\n",
    "\t\t\tnum_metric=num_metric, \\\n",
    "\t\t\tcat_metric=cat_metric)\n",
    "assert not X_train.isna().any().any(), \"ABANS ENCODE 1.\"\n",
    "encode_variables(data=X_train, save_to_csv=False)\n",
    "assert not X_train.isna().any().any(), \"DESPRES ENCODE 1.\"\n",
    "decode_variables(data=X_train, ohe_mapping=ohe_mapping, original_columns_order=original_columns_order)\n",
    "assert not X_train.isna().any().any(), \"DESPRES DECODE 1.\"\n",
    "balance_target_classes(X_data=X_train, y_data=y_train, method='smote', random_state=random_state)\n",
    "assert not X_train.isna().any().any(), \"DESPRES BALANCE.\"\n",
    "\n",
    "_ = knn_prediction(X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, \\\n",
    "\t\t\t   \tk_list=[1, 2, 3, 5, 10, 15, 20, 25, 50], \\\n",
    "\t\t\t\tscorer_cross_val=cat_metric, \\\n",
    "\t\t\t\tfolds_cross_val=folds, \\\n",
    "\t\t\t\tencode=False, \\\n",
    "\t\t\t\tplot_confusion_matrix=True, \\\n",
    "\t\t\t\tprint_scores=True)\n",
    "#_ = dt_prediction(X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, \\criterion_list=['entropy', 'gini'], max_depth_list=[None, 2, 3, 5, 10, 15, 20, 25, 50], min_samples_split_list=[2, 3, 5, 10, 15, 20, 25, 50], min_samples_leaf_list=[1, 2, 3, 5, 10, 15, 20, 25, 50], random_state=random_state, folds_cross_val=folds, scorer_cross_val=cat_metric, plot_confusion_matrix=False, print_scores=True)\n",
    "#_ = svm_prediction(X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, kernel_list=['linear', 'poly', 'rbf', 'sigmoid'], C_list=[0.1, 0.5, 1, 2, 5, 10, 20, 50, 100], gamma_list=['scale', 'auto'], scorer_cross_val='cat_metric', folds_cross_val=folds, print_scores=True, plot_confusion_matrix=False, random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Execució d'experiments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "\t\trandom_state: int = 42,\n",
    "\t\tremove_outliers: bool = True,\n",
    "\t\toutliers_factor: int = 3,\n",
    "\t\tremove_last_rows: bool = False,\n",
    "\t\ttest_size: float = 0.15,\n",
    "\t\tscaler: str = '',\n",
    "\t\timputer_cross_val_folds: int = 5,\n",
    "\t\timputer_num_metric: str = 'r2',\n",
    "\t\timputer_cat_metric: str = 'accuracy',\n",
    "\t\tbalancer: str = '',\n",
    "\t\tpredictor: str = 'knn',\n",
    "\t\tknn_params: dict = {'k_list': [1, 2, 3, 5, 10, 15, 20, 25, 50], \\\n",
    "\t\t\t\t\t  \t\t'scorer_cross_val': 'accuracy', \\\n",
    "\t\t\t\t\t\t\t'folds_cross_val': 5, \\\n",
    "\t\t\t\t\t\t\t'encode': False, \\\n",
    "\t\t\t\t\t\t\t'plot_confusion_matrix': True, \\\n",
    "\t\t\t\t\t\t\t'print_scores': True},\n",
    "\t\tdt_params: dict = {'criterion_list': ['entropy', 'gini'], \\\n",
    "\t\t\t\t\t\t\t'max_depth_list': [None, 2, 3, 5, 10, 15, 20, 25, 50], \\\n",
    "\t\t\t\t\t\t\t'min_samples_split_list': [2, 3, 5, 10, 15, 20, 25, 50], \\\n",
    "\t\t\t\t\t\t\t'min_samples_leaf_list': [1, 2, 3, 5, 10, 15, 20, 25, 50], \\\n",
    "\t\t\t\t\t\t\t'random_state': 42, \\\n",
    "\t\t\t\t\t\t\t'folds_cross_val': 5, \\\n",
    "\t\t\t\t\t\t\t'scorer_cross_val': 'accuracy', \\\n",
    "\t\t\t\t\t\t\t'plot_confusion_matrix': False, \\\n",
    "\t\t\t\t\t\t\t'print_scores': True},\n",
    "\t\tsvm_params: dict = {'kernel_list': ['linear', 'poly', 'rbf', 'sigmoid'], \\\n",
    "\t\t\t\t\t  \t\t'C_list': [0.1, 0.5, 1, 2, 5, 10, 20, 50, 100], \\\n",
    "\t\t\t\t\t\t\t'gamma_list': ['scale', 'auto'], \\\n",
    "\t\t\t\t\t\t\t'scorer_cross_val': 'accuracy', \\\n",
    "\t\t\t\t\t\t\t'folds_cross_val': 5, \\\n",
    "\t\t\t\t\t\t\t'print_scores': True, \\\n",
    "\t\t\t\t\t\t\t'plot_confusion_matrix': False, \\\n",
    "\t\t\t\t\t\t\t'random_state': 42}\n",
    "):\n",
    "\t\"\"\"\n",
    "\tRealitza un experiment complet amb els paràmetres especificats.\n",
    "\t\"\"\"\n",
    "\n",
    "\tassert 0 < test_size < 1, \"El paràmetre 'test_size' ha de ser un valor entre 0 i 1.\"\n",
    "\tassert scaler in {'', 'standard', 'minmax'}, \"El paràmetre 'scaler' ha de ser un dels següents: {'', 'standard', 'minmax'}\"\n",
    "\tassert outliers_factor > 0, \"El paràmetre 'outliers_factor' ha de ser un valor positiu.\"\n",
    "\tassert imputer_cross_val_folds > 1, \"El paràmetre 'imputer_cross_val_folds' ha de ser un valor positiu.\"\n",
    "\tassert balancer in {'', 'oversample', 'undersample', 'smote', 'smoteenn'}, \"El paràmetre 'balancer' ha de ser un dels següents: {'', 'oversample', 'undersample', 'smote', 'smoteenn'}\"\n",
    "\tassert predictor in {'knn', 'dt', 'svm'}, \"El paràmetre 'predictor' ha de ser un dels següents: {'knn', 'dt', 'svm'}\"\n",
    "\n",
    "\tload_dataset(save_to_csv=False)\n",
    "\tinitial_preprocessing(data=data, save_to_csv=False)\n",
    "\tif remove_outliers:\n",
    "\t\tdelete_outliers(data=data, factor=outliers_factor, plots=False, save_to_csv=False)\n",
    "\tif remove_last_rows:\n",
    "\t\tdelete_last_rows(data=data, save_to_csv=False)\n",
    "\tsplit_dataset(data=data, test_size=test_size, random_state=random_state)\n",
    "\tif scaler:\n",
    "\t\tscale_variables(data=X_train, scaler=scaler, save_to_csv=False)\n",
    "\t\tscale_variables(data=X_test, scaler=scaler, save_to_csv=False)\n",
    "\timpute_data(data_to_impute=X_test, \\\n",
    "\t\t\t\tnumerical_imputer='best', \\\n",
    "\t\t\t\tcategorical_imputer='best', \\\n",
    "\t\t\t\tsave_to_csv=False, \\\n",
    "\t\t\t\trandom_state=random_state, \\\n",
    "\t\t\t\tfolds_cross_val=imputer_cross_val_folds, \\\n",
    "\t\t\t\tnum_metric=imputer_num_metric, \\\n",
    "\t\t\t\tcat_metric=imputer_cat_metric)\n",
    "\timpute_data(data_to_impute=X_train, \\\n",
    "\t\t\t\tnumerical_imputer='best', \\\n",
    "\t\t\t\tcategorical_imputer='best', \\\n",
    "\t\t\t\tsave_to_csv=False, \\\n",
    "\t\t\t\trandom_state=random_state, \\\n",
    "\t\t\t\tfolds_cross_val=imputer_cross_val_folds, \\\n",
    "\t\t\t\tnum_metric=imputer_num_metric, \\\n",
    "\t\t\t\tcat_metric=imputer_cat_metric)\n",
    "\tif balancer:\n",
    "\t\tbalance_target_classes(X_data=X_train, y_data=y_train, method=balancer, random_state=random_state)\n",
    "\n",
    "\tif predictor == 'knn':\n",
    "\t\tprediction_scores = knn_prediction(X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, **knn_params)\n",
    "\telif predictor == 'dt':\n",
    "\t\tprediction_scores = dt_prediction(X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, **dt_params)\n",
    "\telif predictor == 'svm':\n",
    "\t\tprediction_scores = svm_prediction(X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, **svm_params)\n",
    "\t\n",
    "\treturn prediction_scores\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_experiment(executions_per_experiment: int = 10):\n",
    "\t\"\"\"\n",
    "\tProva totes les combinacions de paràmetres i retorna la millor.\n",
    "\t\"\"\"\n",
    "\tfrom itertools import product\n",
    "\n",
    "\t# Paràmetres\n",
    "\trandom_state_list = [42, 123, 456, 789]\n",
    "\tremove_outliers_list = [True, False]\n",
    "\toutliers_factor_list = [3, 5, 10]\n",
    "\tremove_last_rows_list = [True, False]\n",
    "\ttest_size_list = [0.15, 0.2, 0.25]\n",
    "\tscaler_list = ['', 'standard', 'minmax']\n",
    "\timputer_cross_val_folds_list = [5, 10]\n",
    "\timputer_num_metric_list = ['r2']\n",
    "\timputer_cat_metric_list = ['accuracy', 'f1_micro', 'f1_macro', 'f1_weighted', 'precision_micro', 'precision_macro', 'precision_weighted', 'recall_micro', 'recall_macro', 'recall_weighted']\n",
    "\tbalancer_list = ['', 'oversample', 'undersample', 'smote', 'smoteenn']\n",
    "\tpredictor_list = ['knn', 'dt', 'svm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BONUS 1: Explainable Boosting Machine (EBM)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BONUS 2: Anàlisi No Supervisat**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
