{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IAA - PRÀCTICA: MAIN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Instal·lar llibreries necessàries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../assets/requirements.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importar llibreries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_dependencies():\n",
    "\tglobal pd, np, plt, sns, skl\n",
    "\n",
    "\timport pandas as pd\n",
    "\timport numpy as np\n",
    "\timport matplotlib.pyplot as plt\n",
    "\timport seaborn as sns\n",
    "\timport sklearn as skl\n",
    "\n",
    "#import_dependencies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Llegir les dades (Cirrhosis Dataset)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(save_to_csv: bool = True):\n",
    "\tglobal data\n",
    "\tfrom ucimlrepo import fetch_ucirepo \n",
    "\t\n",
    "\t# Fetch dataset\n",
    "\tcirrhosis_patient_survival_prediction = fetch_ucirepo(id=878)\n",
    "\n",
    "\tdata = pd.DataFrame(cirrhosis_patient_survival_prediction.data.original)\n",
    "\n",
    "\tif save_to_csv:\n",
    "\t\t# Guardem el dataset per poder-lo visualitzar sencer\n",
    "\t\tdata.to_csv('../assets/data/raw_cirrhosis.csv', index=False)\n",
    "\n",
    "#load_dataset(save_to_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Informació del dataset inicial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preprocessing inicial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_preprocessing(data: pd.DataFrame, save_to_csv: bool = True):\n",
    "\t\"\"\"\n",
    "\tReemplaça els valors 'NaNN' per NaN, assigna els tipus de dades correctes a cada columna i renombra les classes d'algunes variables per una millor comprensió.\n",
    "\t\"\"\"\n",
    "\t# Reemplaçar l'string 'NaNN' per NaN\n",
    "\tdata.replace(to_replace=['NaNN', '', pd.NA], value=np.nan, inplace=True)\n",
    "\n",
    "\t# Assignem els tipus de dades correctes a cada columna\n",
    "\tint64_variables = ['N_Days', 'Age', 'Cholesterol', 'Copper', 'Tryglicerides', 'Platelets']\n",
    "\tfloat64_variables = ['Bilirubin', 'Albumin', 'Alk_Phos', 'SGOT', 'Prothrombin']\n",
    "\tcategory_variables = ['ID', 'Status', 'Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema', 'Stage']\n",
    "\tboolean_variables = ['Ascites', 'Hepatomegaly', 'Spiders']\n",
    "\n",
    "\tdata[int64_variables] = data[int64_variables].astype('Int64')\n",
    "\tdata[float64_variables] = data[float64_variables].astype('float64')\n",
    "\tdata[category_variables] = data[category_variables].astype('category')\n",
    "\n",
    "\tglobal original_column_types\n",
    "\n",
    "\toriginal_column_types = {col: str(data[col].dtype) for col in data.columns} # Guardem els tipus de dades de cada columna\n",
    "\n",
    "\t# Renombrem les classes d'algunes variables per una millor comprensió\n",
    "\tdata['Status'] = data['Status'].replace({'D': 'Dead', 'C': 'Alive', 'CL': 'LiverTransplant'})\n",
    "\tdata[boolean_variables] = data[boolean_variables].replace({'Y': 1, 'N': 0})\n",
    "\tdata['Edema'] = data['Edema'].replace({'N': 'NoEdema', 'S': 'EdemaResolved', 'Y': 'EdemaPersistent'})\n",
    "\n",
    "\tif save_to_csv:\n",
    "\t\t# Guardem el dataset\n",
    "\t\tdata.to_csv('../assets/data/initial_preprocessing_cirrhosis.csv', index=False)\n",
    "\n",
    "#initial_preprocessing(data=data, save_to_csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Anàlisis inicial de les variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estudi de les variables numèriques\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estadístiques de les variables categòriques\n",
    "data.describe(include='category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_vars_histograms(data: pd.DataFrame):\n",
    "    # Visualització de les distribucions de les variables numèriques en una sola figura\n",
    "    numerical_columns = data.select_dtypes(include=['Int64', 'float64']).columns\n",
    "\n",
    "    num_rows = int(np.ceil(len(numerical_columns) / 2))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, num_rows * 4))\n",
    "\n",
    "    for i, col in enumerate(numerical_columns):\n",
    "        ax = fig.add_subplot(num_rows, 2, i + 1)\n",
    "        \n",
    "        sns.histplot(data[col], edgecolor=\"k\", linewidth=1.5, kde=True)\n",
    "        \n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        \n",
    "        ax.set_title(f'Distribució de la variable numèrica {col}')\n",
    "        ax.set_xlabel(col)\n",
    "        ax.set_ylabel('Freqüència')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#numerical_vars_histograms(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_vars_countplots(data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Visualització de les distribucions de les variables categòriques en una sola figura (menys ID).\n",
    "    \"\"\"\n",
    "    # Visualització de les distribucions de les variables categòriques en una sola figura (menys ID)\n",
    "    categorical_columns = data.select_dtypes(include=['category']).columns.drop(['ID'])\n",
    "    num_rows = int(np.ceil(len(categorical_columns) / 2))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, num_rows * 4))\n",
    "\n",
    "    for i, col in enumerate(categorical_columns):\n",
    "        ax = fig.add_subplot(num_rows, 2, i + 1)\n",
    "        \n",
    "        sns.countplot(data=data, x=col, ax=ax, hue=col, legend=False)\n",
    "        \n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        \n",
    "        ax.set_title(f'Distribució de la variable categòrica {col}')\n",
    "        ax.set_xlabel(col)\n",
    "        ax.set_ylabel('Quantitat')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#categorical_vars_countplots(data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tractament d'outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_iqr_factors(data: pd.DataFrame, factors: list = [1.5, 1.75, 2, 2.25, 2.5, 2.75, 3, 3.25, 3.5, 3.75, 4, 4.25, 4.5, 4.75, 5]):\n",
    "\t\"\"\"\n",
    "\tCompara diferents factors que multipliquen al IQR per a determinar els outliers i realitza un gràfic evolutiu per comparar-los.\n",
    "\t\"\"\"\n",
    "\tnumerical_columns = data.select_dtypes(include=['Int64', 'float64']).columns\n",
    "\n",
    "\tplt.figure(figsize=(10, 6))\n",
    "\n",
    "\t# Dictionary to store outlier percentages for each factor and column\n",
    "\toutlier_percentages = {col: [] for col in numerical_columns}\n",
    "\ttotal_percentages = [set() for _ in range(len(factors))]\n",
    "\n",
    "\tfor col in numerical_columns:\n",
    "\t\tQ1 = data[col].quantile(0.25)\n",
    "\t\tQ3 = data[col].quantile(0.75)\n",
    "\t\tIQR = Q3 - Q1\n",
    "\n",
    "\t\tfor f_id, factor in enumerate(factors):\n",
    "\t\t\toutliers_mask = ((data[col] < (Q1 - factor * IQR)) | (data[col] > (Q3 + factor * IQR)))\n",
    "\t\t\ttotal_percentages[f_id].update(data.index[outliers_mask])\n",
    "\t\t\toutliers_percentage = np.mean(outliers_mask) * 100\n",
    "\t\t\toutlier_percentages[col].append(outliers_percentage)\n",
    "\n",
    "\ttotal_percentages = [(len(outliers) / len(data)) * 100 for outliers in total_percentages]\n",
    "\t\t\t\n",
    "\t# Plotting the results\n",
    "\tfor col, percentages in outlier_percentages.items():\n",
    "\t\tplt.plot(factors, percentages, label=col)\n",
    "\tplt.plot(factors, total_percentages, label='Total', linestyle='--', color='black')\n",
    "\n",
    "\tplt.xlabel('Factor multiplicatiu del IQR')\n",
    "\tplt.ylabel('Percentage d\\'outliers (%)')\n",
    "\tplt.title('Percentatge d\\'outliers de cada variable numèrica per a diferents factors multiplicatius del IQR')\n",
    "\tplt.xticks(factors)\n",
    "\t\n",
    "\tplt.legend()\n",
    "\tplt.grid(True)\n",
    "\tplt.show()\n",
    "\n",
    "#compare_iqr_factors(data=data, factors=[1.5, 1.75, 2, 2.25, 2.5, 2.75, 3, 3.25, 3.5, 3.75, 4, 4.25, 4.5, 4.75, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datset amb outliers: 418 files i 20 columnes.\n",
      "Nombre total d'outliers únics eliminats: 70 (16.75% de tot el dataset).\n",
      "Dataset sense outliers: 348 files i 20 columnes.\n"
     ]
    }
   ],
   "source": [
    "def delete_outliers(data: pd.DataFrame, factor: float = 1.5, plots: bool = True, save_to_csv: bool = True):\n",
    "    \"\"\"\n",
    "    Funció que detecta, visualitza i elimina els outliers d'un dataset. El factor multiplica el IQR per a determinar quins valors són outliers.\n",
    "    \"\"\"\n",
    "    # Detecció, visualització i eliminació d'outliers\n",
    "    numerical_columns = data.select_dtypes(include=['Int64', 'float64']).columns\n",
    "\n",
    "    outliers_indices = []\n",
    "\n",
    "    for col in numerical_columns:\n",
    "        Q1 = data[col].quantile(0.25)\n",
    "        Q3 = data[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - factor * IQR\n",
    "        upper_bound = Q3 + factor * IQR\n",
    "        outliers_mask = ((data[col] < lower_bound) | (data[col] > upper_bound))\n",
    "        outliers = data[col][outliers_mask]\n",
    "        non_outliers = data[col][~outliers_mask]\n",
    "\n",
    "        outliers_indices.extend(data[col][outliers_mask].index.tolist())\n",
    "        \n",
    "        if plots:\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(8, 6))\n",
    "\n",
    "            # Boxplot con los outliers originales\n",
    "            sns.boxplot(ax=axes[0, 0], y=data[col], orient='v')\n",
    "            axes[0, 0].scatter(x=[0]*len(outliers), y=outliers, color='red', marker='o')\n",
    "            axes[0, 0].set_title(f'Boxplot de {col} con outliers ({factor}x IQR)')\n",
    "\n",
    "            # Histograma con línea vertical para outliers\n",
    "            sns.histplot(ax=axes[0, 1], data=data, x=col, kde=True)\n",
    "            if (data[col] < lower_bound).any():\n",
    "                axes[0, 1].axvline(x=lower_bound, color='red', linestyle='dashed')\n",
    "            if (data[col] > upper_bound).any():\n",
    "                axes[0, 1].axvline(x=upper_bound, color='red', linestyle='dashed')\n",
    "            axes[0, 1].set_title(f'Histograma de {col}')\n",
    "            axes[0, 1].set_xlabel(col)\n",
    "            axes[0, 1].set_ylabel('Frecuencia')\n",
    "\n",
    "            # Boxplot sin los outliers\n",
    "            sns.boxplot(ax=axes[1, 0], y=non_outliers, orient='v')\n",
    "            axes[1, 0].set_title(f'Boxplot de {col} sin outliers')\n",
    "\n",
    "            # Histograma sin los outliers\n",
    "            sns.histplot(ax=axes[1, 1], data=data[~outliers_mask], x=col, kde=True)\n",
    "            axes[1, 1].set_title(f'Histograma de {col} sin outliers')\n",
    "\n",
    "            percent_outliers = len(outliers) / data.shape[0] * 100\n",
    "            fig.text(x=0.5, y=0, s=f'Outliers de {col} ({factor}x IQR): {len(outliers)} ({percent_outliers:.2f}%)', \n",
    "                    ha='center', va='center')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    unique_outliers = len(set(outliers_indices))\n",
    "\n",
    "    print(f\"Datset amb outliers: {data.shape[0]} files i {data.shape[1]} columnes.\")\n",
    "    print(f\"Nombre total d'outliers únics eliminats: {unique_outliers} ({unique_outliers / data.shape[0] * 100:.2f}% de tot el dataset).\")\n",
    "\n",
    "    # Eliminació d'outliers\n",
    "    data.drop(list(set(outliers_indices)), inplace=True)\n",
    "    \n",
    "    print(f\"Dataset sense outliers: {data.shape[0]} files i {data.shape[1]} columnes.\")\n",
    "\n",
    "    if save_to_csv:\n",
    "        # Guardem el dataset\n",
    "        data.to_csv('../assets/data/no_outliers_cirrhosis.csv', index=False)\n",
    "\n",
    "#delete_outliers(data=data, factor=3, plots=False, save_to_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Recodificació de variables categòriques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_variables(data: pd.DataFrame, save_to_csv: bool = True):\n",
    "    \"\"\"\n",
    "    Codifica les variables categòriques que calgui per a poder-les utilitzar en els models de ML. \n",
    "    A més, guarda el mapping per a poder decodificar-les.\n",
    "    Els NaNs es mantenen (en comptes de considerar-los una classe més) per poder imputar-los posteriorment.\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.impute import SimpleImputer\n",
    "\n",
    "    global ohe_mapping, original_columns_order\n",
    "\n",
    "    original_columns_order = data.columns\n",
    "\n",
    "    columns_to_encode = ['Drug', 'Sex', 'Edema', 'Stage'] # Sense la variable 'Status' perquè és la target i, a més, no té valors NaN\n",
    "\n",
    "    na_indexs_per_old_encoded_column = {col: set(data[data[col].isna()].index) for col in columns_to_encode} # Guardem els indexs dels NaNs per a cada columna a codificar\n",
    "    new_encoded_columns_per_old_encoded_column = {col: set() for col in columns_to_encode} # Guardem les classes de cada columna a codificar\n",
    "\n",
    "    # Imputem els NaNs per evitar que es crein columnes innecessàries al fer el OneHotEncoding. Després tornarem a inserir els NaNs\n",
    "    data[columns_to_encode] = SimpleImputer(strategy='most_frequent').fit_transform(data[columns_to_encode])\n",
    "\n",
    "    ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "    data_encoded = ohe.fit_transform(data[columns_to_encode])\n",
    "    encoded_columns = ohe.get_feature_names_out(columns_to_encode)\n",
    "\n",
    "    # Guardem el mapping per a poder decodificar les variables\n",
    "    ohe_mapping = {}\n",
    "    for i, col in enumerate(columns_to_encode):\n",
    "        for category in ohe.categories_[i]:\n",
    "            new_encoded_column_name = f\"{col}_{category}\"\n",
    "            ohe_mapping[new_encoded_column_name] = (col, category)\n",
    "            new_encoded_columns_per_old_encoded_column[col].add(new_encoded_column_name)\n",
    "\n",
    "    data[encoded_columns] = data_encoded\n",
    "    data[encoded_columns] = data[encoded_columns].astype('category')\n",
    "\n",
    "    # Tornem a posar els NaNs per poder imputar-los\n",
    "    for col in columns_to_encode:\n",
    "        for na_index in na_indexs_per_old_encoded_column[col]:\n",
    "            for new_column in new_encoded_columns_per_old_encoded_column[col]:\n",
    "                data.loc[na_index, new_column] = np.nan\n",
    "\n",
    "    # Eliminem les columnes originals\n",
    "    data.drop(columns=columns_to_encode, inplace=True)\n",
    "\n",
    "    if save_to_csv:\n",
    "        # Guardem el dataset\n",
    "        data.to_csv('../assets/data/encoded_cirrhosis.csv', index=False)\n",
    "\n",
    "#encode_variables(data=data, save_to_csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_variables(data: pd.DataFrame, ohe_mapping, original_columns_order):\n",
    "    \"\"\"\n",
    "    Decodifica les variables categòriques que s'hagin codificat anteriorment.\n",
    "    \"\"\"\n",
    "    reconstructed_columns = {}\n",
    "\n",
    "    # Creem les columnes reconstruïdes\n",
    "    for encoded_column in ohe_mapping:\n",
    "        if encoded_column in data.columns:\n",
    "            original_column, category = ohe_mapping[encoded_column]\n",
    "\n",
    "            if original_column not in reconstructed_columns:\n",
    "                reconstructed_columns[original_column] = pd.Series([np.nan] * len(data), index=data.index, dtype='object')\n",
    "\n",
    "            category_rows = data[encoded_column] == 1\n",
    "            reconstructed_columns[original_column].loc[category_rows] = category\n",
    "\n",
    "    # Eliminem les columnes codificades\n",
    "    data.drop(columns=[col for col in ohe_mapping if col in data.columns], inplace=True)\n",
    "\n",
    "    # Inserim les columnes reconstruïdes al DataFrame\n",
    "    for col in reconstructed_columns:\n",
    "        data[col] = reconstructed_columns[col]\n",
    "        data[col] = data[col].astype(original_column_types[col])\n",
    "\n",
    "    # Reordenem les columnes perquè quedin igual que a l'original\n",
    "    data = data.reindex(columns=original_columns_order)\n",
    "\n",
    "#decode_variables(data=data, ohe_mapping=ohe_mapping, original_columns_order=original_columns_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Escalar variables numèriques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_variables(data: pd.DataFrame, scaler: str = 'standard', save_to_csv: bool = True):\n",
    "\tassert scaler in ['standard', 'minmax'], \"El paràmetre 'scaler' ha de ser 'standard' o 'minmax'.\"\n",
    "\t\"\"\"\n",
    "\tEscala les variables numèriques.\n",
    "\t\"\"\"\n",
    "\tfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\tnumerical_columns = data.select_dtypes(include=['Int64', 'float64']).columns\n",
    "\n",
    "\tsc = StandardScaler() if scaler == 'standard' else MinMaxScaler()\n",
    "\n",
    "\tdata[numerical_columns] = sc.fit_transform(data[numerical_columns])\n",
    "\n",
    "\tif save_to_csv:\n",
    "\t\t# Guardem el dataset\n",
    "\t\tdata.to_csv('../assets/data/scaled_cirrhosis.csv', index=False)\n",
    "\n",
    "# scale_variables(data=data, scaler='standard', save_to_csv=True)\n",
    "#scale_variables(data=data, scaler='minmax', save_to_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Partició del dataset en train/test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (295, 20)\n",
      "Test shape: (53, 20)\n"
     ]
    }
   ],
   "source": [
    "def split_dataset(data: pd.DataFrame, test_size: float = 0.15, stratify: bool = True, random_state: int = 42):\n",
    "\t\"\"\"\n",
    "\tParticiona el dataset en train i test.\n",
    "\t\"\"\"\n",
    "\tglobal train, test, X_train, y_train, X_test, y_test\n",
    "\n",
    "\tfrom sklearn.model_selection import train_test_split\n",
    "\n",
    "\ttrain, test = train_test_split(data, test_size=test_size, random_state=random_state, stratify=data['Status']) \\\n",
    "\t\tif stratify else train_test_split(data, test_size=test_size, random_state=random_state)\n",
    "\n",
    "\tprint(f\"Train shape: {train.shape}\")\n",
    "\tprint(f\"Test shape: {test.shape}\")\n",
    "\n",
    "\t# 'Status' és la variable target\n",
    "\tX_train = train.drop(columns=['Status'])\n",
    "\ty_train = train['Status']\n",
    "\tX_test = test.drop(columns=['Status'])\n",
    "\ty_test = test['Status']\n",
    "\n",
    "#split_dataset(data=data, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imputar els valors faltants (Missings)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimeix les variables que tenen valors NaN, el seu percentatge i el seu tipus de dades\n",
    "for col_train in train.columns:\n",
    "\tif train[col_train].isna().any():\n",
    "\t\tprint(f\"{col_train}: {train[col_train].isna().sum()} NaNs ({train[col_train].isna().sum() / len(train) * 100:.2f}%) ({train[col_train].dtype})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_imputer(data_is_encoded: bool, X_train: pd.DataFrame, random_state: int = 42, print_scores: bool = True, return_best_imputer: bool = True, proportion_to_test_imputation: float = 0.1):\n",
    "    \"\"\"\n",
    "    Prova diferents imputadors, imprimeix els seus resultats i retorna el millor.\n",
    "    \"\"\"\n",
    "    assert proportion_to_test_imputation > 0 and proportion_to_test_imputation < 1, \"El paràmetre 'proportion_to_test_imputation' ha de ser un valor entre 0 i 1.\"\n",
    "    from sklearn.impute import KNNImputer, SimpleImputer\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.metrics import r2_score, accuracy_score\n",
    "\n",
    "    numerical_columns = X_train.select_dtypes(include=['Int64', 'float64']).columns\n",
    "    categorical_columns = X_train.select_dtypes(include=['category']).columns.drop(['ID'])\n",
    "    original_cols_with_na = X_train.columns[X_train.isna().any()]\n",
    "\n",
    "    MixedImputer = ColumnTransformer([\n",
    "        ('numerical', SimpleImputer(strategy='mean'), numerical_columns),\n",
    "        ('categorical', SimpleImputer(strategy='most_frequent'), categorical_columns)\n",
    "    ])\n",
    "\n",
    "    imputers: dict = {'mixed': MixedImputer}\n",
    "\n",
    "    if data_is_encoded:\n",
    "        imputers['knn-1'] = KNNImputer(n_neighbors=1)\n",
    "        imputers['knn-3'] = KNNImputer(n_neighbors=3)\n",
    "        imputers['knn-5'] = KNNImputer(n_neighbors=5)\n",
    "\n",
    "    # from sklearn.experimental import enable_iterative_imputer\n",
    "    # from sklearn.impute import IterativeImputer\n",
    "    # imputers['iterative-10'] = IterativeImputer(max_iter=10, random_state=random_state)\n",
    "    # imputers['iterative-20'] = IterativeImputer(max_iter=20, random_state=random_state)\n",
    "\n",
    "    scores = {}\n",
    "    best_score = float('-inf')\n",
    "    best_imputer = (None, None)\n",
    "    best_imputer_name = 'None'\n",
    "\n",
    "    # Ens quedem només amb les files sense NaNs i sense la variable 'ID' (ja que no aporta informació)\n",
    "    X_train_complete = X_train.dropna().drop(columns=['ID'])\n",
    "\n",
    "    # Creem un dataset amb NaNs aleatoris per imputar\n",
    "    X_train_incomplete = X_train_complete.copy()\n",
    "    for col in original_cols_with_na:\n",
    "        X_train_incomplete.loc[X_train_incomplete.sample(frac=proportion_to_test_imputation, random_state=random_state).index, col] = np.nan\n",
    "\n",
    "    # Imputar i calcular mètriques\n",
    "    for name_imputer, imputer in imputers.items():\n",
    "        # Imputar\n",
    "        imputed_data = imputer.fit_transform(X_train_incomplete)\n",
    "\n",
    "        # Convertir a DataFrame y asegurarse de que las columnas coincidan\n",
    "        if isinstance(imputer, ColumnTransformer):\n",
    "            # Extraer los nombres de las columnas después de la transformación\n",
    "            transformed_columns = [col for name, trans, cols in imputer.transformers if trans != 'drop' for col in cols]\n",
    "            X_train_imputed = pd.DataFrame(imputed_data, columns=transformed_columns, index=X_train_incomplete.index)\n",
    "        else:\n",
    "            # Para otros imputadores, simplemente usa las columnas originales\n",
    "            X_train_imputed = pd.DataFrame(imputed_data, columns=X_train_incomplete.columns, index=X_train_incomplete.index)\n",
    "\n",
    "        # Calcular mètriques\n",
    "        r2_scores = {} # Per a les variables numèriques\n",
    "        acc_scores = {} # Per a les variables categòriques\n",
    "        for col in original_cols_with_na:\n",
    "            if col in numerical_columns:\n",
    "                r2 = r2_score(X_train_complete[col], X_train_imputed[col])\n",
    "                r2_scores[col] = r2\n",
    "            elif col in categorical_columns:\n",
    "                acc = accuracy_score(X_train_complete[col].astype('category'), np.round(X_train_imputed[col].astype('float64')).astype('category'))\n",
    "                acc_scores[col] = acc\n",
    "\n",
    "        overall_score = np.mean(list(r2_scores.values()) + list(acc_scores.values()))\n",
    "        scores[name_imputer] = {'categorical': acc_scores, 'numerical': r2_scores, 'overall': overall_score}\n",
    "\n",
    "        # Guardar el millor imputador\n",
    "        if overall_score > best_score:\n",
    "            best_score = overall_score\n",
    "            best_imputer = imputer\n",
    "            best_imputer_name = name_imputer\n",
    "\n",
    "    # Imprimir els resultats\n",
    "    if print_scores:\n",
    "        for name_imputer, scores_imputer in scores.items():\n",
    "            print(f\"IMPUTER [{name_imputer}]: {scores_imputer['overall']} (overall score)\")\n",
    "            print(f\"\\t*Variables numèriques (R²): {np.mean(list(scores_imputer['numerical'].values()))}\")\n",
    "            for col, s in scores_imputer['numerical'].items():\n",
    "                print(f\"\\t\\t*{col}: {s}\")\n",
    "            print(f\"\\t*Variables categòriques (Accuracy): {np.mean(list(scores_imputer['categorical'].values()))}\")\n",
    "            for col, s in scores_imputer['categorical'].items():\n",
    "                print(f\"\\t\\t*{col}: {s}\")\n",
    "            print()\n",
    "\n",
    "        print(f\"MILLOR IMPUTER OVERALL --> {best_imputer_name} ({best_score})\")\n",
    "    \n",
    "    # Retornar el millor imputador\n",
    "    if return_best_imputer:\n",
    "        return best_imputer_name, best_imputer, best_score\n",
    "\n",
    "#best_imputer(X_train=X_train, random_state=42, print_scores=True, return_best_imputer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_data(data_to_impute: pd.DataFrame, imputer = 'best', save_to_csv: bool = True, random_state: int = 42, encode: bool = True, decode: bool = True, proportion_to_test_imputation: float = 0.1):\n",
    "\t\"\"\"\n",
    "\tImputa els valors NaN del dataset.\n",
    "\t\"\"\"\n",
    "\tfrom sklearn.compose import ColumnTransformer\n",
    "\n",
    "\t# Si no hi ha cap NaN, no cal imputar\n",
    "\tif not data_to_impute.isna().values.any():\n",
    "\t\tprint(\"No hi ha cap NaN al dataset.\")\n",
    "\t\treturn\n",
    "\n",
    "\tif imputer == 'best':\n",
    "\t\tif encode:\n",
    "\t\t\t# Codifiquem les variables categòriques\n",
    "\t\t\tencode_variables(data=data_to_impute, save_to_csv=False)\n",
    "\n",
    "\t\tname_imputer, imputer, score_imputer = best_imputer(data_is_encoded=encode, X_train=X_train, random_state=random_state, print_scores=False, return_best_imputer=True, proportion_to_test_imputation=proportion_to_test_imputation)\t\n",
    "\n",
    "\t\tprint(f\"IMPUTADOR SELECCIONAT: {name_imputer} ({score_imputer} overall score imputant en X_train)\")\n",
    "\t\n",
    "\timputed_data = imputer.fit_transform(data_to_impute)\n",
    "\n",
    "\tif isinstance(imputer, ColumnTransformer):\n",
    "\t\t# Extraer los nombres de las columnas después de la transformación\n",
    "\t\ttransformed_columns = [col for name, trans, cols in imputer.transformers if trans != 'drop' for col in cols]\n",
    "\t\tdata_to_impute = pd.DataFrame(imputed_data, columns=transformed_columns, index=data_to_impute.index)\n",
    "\telse:\n",
    "\t\t# Para otros imputadores, simplemente usa las columnas originales\n",
    "\t\tdata_to_impute = pd.DataFrame(imputed_data, columns=data_to_impute.columns, index=data_to_impute.index)\n",
    "\n",
    "\t# Comprovem que ja no hi hagi NaNs\n",
    "\tif data_to_impute.isna().values.any():\n",
    "\t\traise Exception(\"Per algun motiu desconegut, encara hi ha NaNs al dataset imputat.\")\n",
    "\t\n",
    "\tif decode:\n",
    "\t\t# Decodifiquem les variables categòriques\n",
    "\t\tdecode_variables(data=data_to_impute, ohe_mapping=ohe_mapping, original_columns_order=original_columns_order)\n",
    "\t\n",
    "\tif save_to_csv:\n",
    "\t\t# Guardem el dataset\n",
    "\t\tdata_to_impute.to_csv('../assets/data/imputed_cirrhosis.csv', index=False)\n",
    "\n",
    "#impute_data(data_to_impute=X_train, imputer='best', save_to_csv=True, random_state=42, encode=True, decode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datset amb outliers: 418 files i 20 columnes.\n",
      "Nombre total d'outliers únics eliminats: 70 (16.75% de tot el dataset).\n",
      "Dataset sense outliers: 348 files i 20 columnes.\n",
      "Train shape: (295, 20)\n",
      "Test shape: (53, 20)\n",
      "IMPUTADOR SELECCIONAT: knn-5 (0.6441688206303966 overall score imputant en X_train)\n",
      "IMPUTADOR SELECCIONAT: knn-5 (0.6441688206303966 overall score imputant en X_train)\n"
     ]
    }
   ],
   "source": [
    "# Pipeline per realizar tot un experiment\n",
    "import random\n",
    "load_dataset(save_to_csv=False)\n",
    "initial_preprocessing(data=data, save_to_csv=False)\n",
    "delete_outliers(data=data, factor=3, plots=False, save_to_csv=False)\n",
    "#encode_variables(data=data, save_to_csv=False)\n",
    "scale_variables(data=data, scaler='minmax', save_to_csv=False)\n",
    "split_dataset(data=data, test_size=0.15, random_state=random.randint(0, 1000))\n",
    "impute_data(data_to_impute=X_train, imputer='best', save_to_csv=False, random_state=42, encode=True, decode=True, proportion_to_test_imputation=0.5)\n",
    "impute_data(data_to_impute=X_test, imputer='best', save_to_csv=False, random_state=42, encode=True, decode=True, proportion_to_test_imputation=0.5)\n",
    "#decode_variables(data=X_train, ohe_mapping=ohe_mapping, original_columns_order=original_columns_order)\n",
    "#decode_variables(data=X_test, ohe_mapping=ohe_mapping, original_columns_order=original_columns_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_con_imputacion(X, y):\n",
    "    from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "    kf = KFold(n_splits=5)  # Ajustar según necesidades\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        imputacion = mejor_imputacion(X_train, y_train)[0][1]\n",
    "        # Aplicar la mejor imputación y entrenar el modelo\n",
    "        \n",
    "        X_train_imputed = imputacion.fit_transform(X_train)\n",
    "\n",
    "        # Evaluar el modelo en X_test, y_test..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Correlacions entre variables numèriques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_vars_correlations(data: pd.DataFrame):\n",
    "\t\"\"\"\n",
    "\tVisualitza la correlació entre les variables numèriques.\n",
    "\t\"\"\"\n",
    "\n",
    "\tnumerical_columns = data.select_dtypes(include=['Int64', 'float64']).columns\n",
    "\n",
    "\tplt.figure(figsize=(10, 6))\n",
    "\n",
    "\tsns.heatmap(data[numerical_columns].corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "\n",
    "\tplt.title('Correlació entre les variables numèriques')\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()\n",
    "\n",
    "\n",
    "numerical_vars_correlations(data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1r Model: K-Nearest Neighbors (KNN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2n Model: Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3r Model: Support Vector Machine (SVM)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
